{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LC_algorithm_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jd9Y5YEKJCYh",
        "08XSsmyEJG08",
        "s3FIQ2WWAfGw",
        "ZJlVuYlAAeqh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsainez/examples/blob/master/LC_algorithm_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysApKYweEtY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c9c268-f3db-4f56-c5a7-1a449c21d768"
      },
      "source": [
        "! git clone https://github.com/UCMerced-ML/LC-model-compression"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LC-model-compression'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 166 (delta 63), reused 142 (delta 42), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (166/166), 3.19 MiB | 5.32 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evE8fzBE5tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14905a01-04f3-4a60-86d0-3d6b8bd9d1b2"
      },
      "source": [
        "! pip3 install -e ./LC-model-compression"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/LC-model-compression\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from lc==0.1) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from lc==0.1) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from lc==0.1) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from lc==0.1) (0.11.1+cu111)\n",
            "Requirement already satisfied: scikit-learn>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from lc==0.1) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->lc==0.1) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->lc==0.1) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->lc==0.1) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.2->lc==0.1) (7.1.2)\n",
            "Installing collected packages: lc\n",
            "  Running setup.py develop for lc\n",
            "Successfully installed lc-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZWghZ11EXAD"
      },
      "source": [
        "## IMPORTANT!\n",
        "At this point you need to restart the runtime by doing \"Runtime => Restart Runtime\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUa0UHVaFId6"
      },
      "source": [
        "import lc\n",
        "from lc.torch import ParameterTorch as Param, AsVector, AsIs\n",
        "from lc.compression_types import ConstraintL0Pruning, LowRank, RankSelection, AdaptiveQuantization\n",
        "from lc.models.torch import lenet300_classic, lenet300_modern_drop, lenet300_modern\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "torch.set_num_threads(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ullumXeERo6"
      },
      "source": [
        "## Data\n",
        "We use a subset of the MNIST dataset. The dataset contains subtracted 28x28 grayscale images with digits 0, 2, 5, 6, and 7. The images are normalized to have grayscale value 0 to 1 and then mean is subtracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk8La6_4EkLE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "106e4d7f-7a37-44d5-f5ce-084119b07626"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "def show_MNIST_images():\n",
        "    train_data_th = datasets.MNIST(root='./datasets', download=True, train=True)\n",
        "    data_train = np.array(train_data_th.data[:])\n",
        "    targets = np.array(train_data_th.targets)\n",
        "    images_to_show = 5\n",
        "    random_indexes = np.random.randint(data_train.shape[0], size=images_to_show)\n",
        "    for i,ind in enumerate(random_indexes):\n",
        "        plt.subplot(1,images_to_show,i+1)\n",
        "        plt.imshow(data_train[ind], cmap='gray')\n",
        "        plt.xlabel(targets[ind])\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "show_MNIST_images()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 5 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPxCPbyYEkqp"
      },
      "source": [
        "def data_loader(batch_size=2048, n_workers=4):\n",
        "    train_data_th = datasets.MNIST(root='./datasets', download=True, train=True)\n",
        "    test_data_th = datasets.MNIST(root='./datasets', download=True, train=False)\n",
        "\n",
        "    # Get subset of digits that we will be using\n",
        "    indices = (train_data_th.targets == 0) | (train_data_th.targets == 2) | (train_data_th.targets == 5) | (train_data_th.targets == 6) | (train_data_th.targets == 7)\n",
        "    train_data, train_targets = train_data_th.data[indices], train_data_th.targets[indices]\n",
        "\n",
        "    indices = (test_data_th.targets == 0) | (test_data_th.targets == 2) | (test_data_th.targets == 5) | (test_data_th.targets == 6) | (test_data_th.targets == 7)\n",
        "    test_data, test_targets = test_data_th.data[indices], test_data_th.targets[indices]\n",
        "\n",
        "    # Change labels to be in range 0 - C-1 so cross entropy function works\n",
        "    for i, digit in enumerate([0,2,5,6,7]):\n",
        "        train_targets = torch.where(train_targets == digit, i, train_targets)\n",
        "        test_targets = torch.where(test_targets == digit, i, test_targets)\n",
        "\n",
        "    data_train = np.array(train_data[:]).reshape([-1, 28 * 28]).astype(np.float32)\n",
        "    data_test = np.array(test_data[:]).reshape([-1, 28 * 28]).astype(np.float32)\n",
        "\n",
        "    data_train = (data_train / 255)\n",
        "    dtrain_mean = data_train.mean(axis=0)\n",
        "    data_train -= dtrain_mean\n",
        "    data_test = (data_test / 255).astype(np.float32)\n",
        "    data_test -= dtrain_mean\n",
        "\n",
        "    train_data = TensorDataset(torch.from_numpy(data_train), train_targets)\n",
        "\n",
        "    # Create validation set\n",
        "    val_split = int(0.3 * len(train_data))\n",
        "    train_data, val_data = random_split(train_data, [len(train_data) - val_split, val_split], generator=torch.Generator().manual_seed(1778))\n",
        "\n",
        "    test_data = TensorDataset(torch.from_numpy(data_test), test_targets)\n",
        "\n",
        "    train_loader = DataLoader(train_data, num_workers=n_workers, batch_size=batch_size, shuffle=True,)\n",
        "    val_loader = DataLoader(val_data, num_workers=n_workers, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_data, num_workers=n_workers, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2SmI3FArjFb"
      },
      "source": [
        "## Our Subset of MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "oG6pJVsdU03p",
        "outputId": "71428fb5-7089-46b0-97b8-a5f3c4547435"
      },
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "label_map = {0: 0, 1: 2, 2: 5, 3: 6, 4: 7}\n",
        "def show_subset_MNIST_images(images_to_show):\n",
        "    first_batch, labels = next(iter(data_loader(images_to_show)[0]))\n",
        "    data_train = np.array(first_batch[:]).reshape(-1, 28, 28)\n",
        "    targets = np.array(labels)\n",
        "    for i in range(images_to_show):\n",
        "        plt.subplot(1,images_to_show,i+1)\n",
        "        plt.imshow(data_train[i], cmap='gray')\n",
        "        plt.xlabel(str(label_map[targets[i]]) + \" (label: \" + str(targets[i]) + \")\")\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "show_subset_MNIST_images(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAB8CAYAAACG/9HcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de/RVVdnvvzMvgPzAULmJJJp4TUWOmKJDAQ1FAUEzrLxgr9bJk728+ebQofV23rca1qnUoWZ2UgE78QKRZhiZl8iEDEQIUEEIAa0IBUXwUpnr/PHbzL7zce+HvX7Xvff6fsZg8Oz9zL322utZc675m89lhizLIIQQQghRFN7X2ScghBBCCNGRaPIjhBBCiEKhyY8QQgghCoUmP0IIIYQoFJr8CCGEEKJQaPIjhBBCiEKxe57G3bp1y3r06NFe5yJ2wfbt2/HWW2+FtjhW165dZctOZPv27Xj77bfbxJZAsz27d+/eVocTOXnjjTfazJ5du3bNmpqa2uJQogXs2LGjzWzZpUsX2bKT2bp16ytZlvW27+ea/PTo0QPnn39+252VyMWcOXPa7Fg9evTAhAkT2ux4Ih/3339/mx6ve/fuGDNmTJseU1TPvHnz2uxYTU1NGDt2bJsdT+Rj7ty5bXaspqYmnHnmmW12PJGfGTNmbCj3fq7JTyNjiz2G0GZ/lAshhBCihlDMjxBCCCEKhSY/QgghhCgUmvwIIYQQolDUdMxPtZuu2nb8upKcBxv/w6893a6OUySqvfbvvvtui45R7fHz2Ot976v8t0GRbbkrPBsyXt/07NnS/sd4ti0yLe1jni29+4HtoL7YcbTUlm0xztZK36uNsxBCCCGE6CA0+RFCCCFEoeh0t1dbLKXaZdV//OMfu5TLfY7hpbnddtst0e2+++5l21ldkV1geZa+PVvy6/awJb9m23nHANLf1+i2LEdL7fnOO++U1Vl7Vuv28vqmZ2t7XrWyFN8R2Gs7adKkKO+5556JzrPl4sWLo7xx48Yov/HGG0m7N998s+K58HX3xlKrY1taXRH7Yzm8sbTaZyb3V6D6fmnHUn5tdd490J4Up8cLIYQQQkCTHyGEEEIUDE1+hBBCCFEoOjzmJ08qpRfj8fe//z3Kf/vb3xLdX//617KybcfHsOyxxx5R7tq1a6Lj11bXpUuXKHu+TUsj+KnZftanzDrrR+bXbC8gtZlnSz6GvZZshzy25HvA2tJL3WwEWwLVx/Xk6ZstsaeF7cL9DfDtyfEsNh6IacT4Hy9+cvjw4VHea6+9Eh3bwY6XQ4cOjfKmTZuivH79+qTdo48+GuVFixYlOu5XeWzJ94C1pZc+3yh9cydebKW1F9vSe2a+/fbbZWV7DBsfNnHixChfccUVia5Pnz5R5vgwALjhhhsqnle1pRBaQuP1ciGEEEIIB01+hBBCCFEoOj3V3Uu/85bP33rrrSjb5bBx48aVPcawYcOSdnYpleGlufvuuy/R/eEPf4hyjx49Eh3/BruM66VVt+fyXnuRp/yA59ripVW2K5Cmze7YsaPsZ+zxLWzn7t27J7qmpqayMgB069YtyrzMDrx3yZepR1vuCs+1ZZfX2TY2zZntybJnT3sN+dpbe3J/tOfFLh1rP2tfph7dYF5lXts3V6xYEeVevXoluoULF0b52GOPTXSvvfZalPfdd98o77PPPkm78847L8oHHnhgopszZ06Ut2zZkui4P9r+zX3T2rLa8hX12jfZll6/9MZZW46Ax1bW2fF44MCBUb7xxhsTXd++faNs+z3btl+/fomOSy1Mnz490fHvs+7N1tqv/nq1EEIIIUQr0ORHCCGEEIVCkx8hhBBCFIoOifmpdpsK66Nnn+Xee++d6M4999woDxo0KNFxWy+uwIsb6t27d5Q//elPJ7o77rgjyi+88EKi81L5OQYoT3p0LfmmvdgBfm199GxLr/z99u3bE93rr78eZbaX9Wd7vmGOF7PnxZ/z7k0bW8L2y7MtRi3Z0uLZ0/ZNvqZeXA/bz+pYtvbk77bXkGM8rD29FPxqaenu4rVEnnH2O9/5TpS9mLvvfve7iY7jRA4//PAo9+zZM2k3cuTIKB966KGJjuM9Zs+eneh4LKh2N3EgHWe9bUxquS8yXgkY75lp+yVfT9sv+TXbfMCAAUm7L37xi1G212/btm0Vz4vtYH/PGWecUVF3zz33oBI8zrfElvXRk4UQQggh2ghNfoQQQghRKNrF7ZUnzdJLgWbOPvvs5PWQIUOizCmXQJpWx8dcvnx50u6ZZ56Jsl3OHj9+fJS5OiUAfP7zn4+yTYNfsGBBlPPsOF2ry7He8rnVebbkpVTPtcVLp0CaGss2t0vrvMP0k08+meh4mdirPG3xXFtsP69acK3D14Mr/QKpO9mzJ7s/gNQF8tJLLyW6l19+OcqrVq2KsnVRrVmzJsrPP/98omupO8TbWZpf1+uO79WOs7ZsCLtHrC25b1pXCbedP39+xe9+5JFHonzZZZclupNPPjnKZ555ZqLzXB5ehXVvx/d6sSWT55npuZz5Ofnqq68mOrYllyq49tprk3ZcYsALPfCqw1vYJmPGjEl0d999d8VjtvaZWX93ghBCCCFEK9DkRwghhBCFQpMfIYQQQhSKNov58dLZvR29vTTkL3zhC1G2KXfsv7RxIhzz8Ytf/CLKNuaH0/FsDAC3/cpXvpLouIz3Rz/60UTHfs9nn3020eXxg3Ym1cZReKXVbVkBm3bJ8A7A73//+xMdx/Z453XKKadE+aabbkp0Tz31VJStDfj+83Ygt/emd7/XMl7M0+WXX17xc17arI2549d2uwSOGeA+beOmOObO3kscXzJr1qxE11J71kvfZLx4PNs3+bd7fdPGifDYamP1+DheaQJu96Mf/SjR8T1wzDHHJLrBgwdHecOGDYmO7yP7W+vRlpZqbWlLE3Dsju2XHOdjn5k87n7qU5+Kst2qxIubY7vbuMslS5ZE+bOf/Wyi421nbIq8N7ZqewshhBBCiBxo8iOEEEKIQtEhFZ693b55SW/y5MmJ7pBDDony1q1bE90rr7wSZZsS+cADD0SZl9N5qRRId4C2S7WbNm2K8nXXXZforrzyyiiPGjUq0U2ZMiXKXDkVANavX49K1FJ6O+O5STxbejt9Wx3bxbom99tvvyhzyrNNyWUX6cEHH5zonnvuuYrn7KWpV7t8Xi/VucvB9rRprUcccUSUua8A6bK5TZvl5XbrLuNryjupjxgxImnH7jLbNy+88MIo23TbefPmoRJemQamlm1WbXhBngrP3DetS4yvr3UV8nXq2rVrlO04yxW5rS3vvPPOKNtdwidMmBDlW265JdFVa8t6IU95GB5nrWvXsyW7yGypkPPPPz/K3BetLdnOtu9NnTo1yg8++GCi4/PkYwDA1772tSjb68DnaV18rUUrP0IIIYQoFJr8CCGEEKJQaPIjhBBCiELRITE/HkcffXSUTzvttETHpbpXrlyZ6L71rW9Fee3atYmOd+BuamoqKwPp7r/Wn83+UhtfsmjRoiiPHj060bFPm2MTAODWW2+NsvVtsv+8lmMOPKqN7bLptDfccEOUrQ+b40I47uTqq69O2nGMyMaNGxMd290en/G2sPCw9qrHEvrAe68bx1jZNGeO67EptWxfG1/C9mS7PPbYY0k71l1//fWJrl+/flH+5Cc/meh4KwVvawNrs0bofx5eDInXbxm2HZCmKLNsx1mO+bFp2xw3snDhwkTHMZ9eWrVnL3sPNIJtq40Hstean3e2T51wwglR5nHWxvqtWLEiytOnT090nM5un298D/z6179OdBzLY+0zbty4KP/4xz9OdEp1F0IIIYTIgSY/QgghhCgUHeL28paUJ02aFGW7rMpLejNnzkx0vFu0TcfjZde99947yrw0C6RLojaNjpfrrfvjiSeeiDKn1QPARRddFOWjjjoq0R177LFRXrZsGeqdtlpCZjtYW7IL8xOf+ESUOTUTAH74wx9GecuWLRWP77lCWrpEXs9L6d7u2B5eWjX3F+uuYHty3+T3LbwTPADsv//+UbZjhvfd3u/z0qVryY3ZHmUVuJzEsGHDEh27PlevXp3oeDzllGQ7zjI2/ZrHWeumaY/d2Wu1r3pu2DzwfWz7xqWXXhrl4cOHJzp2b3G151WrViXtbr/99ij/5S9/SXTV2ss+T1vaL1tL7fRqIYQQQogOQJMfIYQQQhQKTX6EEEIIUSjaJeYnTxqpl7bHu6Jzih2Q+g05jQ5Ifc4cQ2JjALydjvm1TYPn7/vVr36V6C6++OIoe+l+tbzzsBcHUu15e+m0XozIwIEDE93YsWOjzNfWxkz95Cc/iXIeW3L6Zx7qdVd3S7Up3l5KrWdP2zc5tofL3Ns4AP4+m27rbZVSrS3ybA1RL1Rryz59+iSvP/OZz0TZGyPt9jz33ntvlLlfWVuyjbztGGzfZDt4v8ezVy2Psx7VxvzY38e2taVWzjrrrCh7/ZKZNm1a8pq3svG2z7DxW/x9J510UqKzcZ5Me/ZLrfwIIYQQolBo8iOEEEKIQtFmbi9epvOqF3u7DdulMl4itTp2x9glPE7x4++z6excudlWsOW21vVjUwgZb5lu5MiRUX7yyScrHqNe8FwH1l68HHv66acnOq4uethhhyU6dnnwkry1AdvSVpD2bGmX+Svh3bf15Cbx3Jjeb7TL695v9txela63XULndlzRGUgrv8+dO7fq8/La1aPbywsh8FyR7OYC0pIDa9asSXTcj7kaPwB8+ctfjvLNN98c5c2bNyft2F62Wj7b9iMf+Uiis+fCVHvfNiJsE9uf2bZHHHFExc95z8wFCxZE2YabtMU4a0vA8L1pnxvVVh9vCVr5EUIIIUSh0ORHCCGEEIWiwzc2tUtXHOHvLXlZeBnNc13wxnl2aZ2X7Xhp1p6LzQjylpdbqqtV8mQADRo0KMqjRo1KdB/84AejbJezP/CBD0TZ2pI3vFy3bl2UOfMLSF1ps2bNSnTsPs2T3eVlIrIbr2/fvonuxRdfrPo7agkvQy9PJk61fZP7o80C4mro1lXC52IrrHt9rFpXiaerpWrPHtaWRx55ZJRtRuUPfvCDKM+ePbviMSdPnpy8njhxYpSvuuqqKH/1q19N2nElYHtt+Rg8VgPAPffcU/FzTEvH4Fqt9myxtuQsyS996UuJjt2I9pnGv9f2yw0bNkT5pptuirLte/zMtDrvmclVxDnrDPCf8w8//HBFXWupj54shBBCCNFGaPIjhBBCiEKhyY8QQgghCkWHxPx4sRNeKhv7DdnPCaR+f+uHZ98jt3vzzTeTdhxz4KXS24ql7DvNkwJcbZXSevFFn3vuucnrs88+O8q2dMCf/vSnKPfv37/iMR966KHk9Zw5c6LMPmbeeRgAJkyYEGVbwfaOO+6Ismdnr4qxtQnvfn3ZZZclussvvxz1iBeP5/VbbwdnL7Xe65ucDmu/+7XXXquoq7asRiOmQ3vj7DnnnBNlG/s4f/78KHu25L4IpDF+bC+O/wHSuL1DDjkk0R1++OFRnjdvXqLj/m7H/yLZ0jJ06NAoDx48ONFxTI6No+NYSy4vYuGK3LZfelWc+V6xY/DHPvaxKNtq0nycRx55JNFt2rQpyrxzQ1uglR8hhBBCFApNfoQQQghRKDrE7eWlzPISm92kkNOj999//0THbhS7VMtLhvx99rsZm/rHri7PrZZnA0+mXlxbXqooL2UCwLZt26JsrzVfz6VLlya6n/3sZ1H20sT5/rj99tsT3UsvvRRl644bPnx4lO+///5Ex5vn2qV1Thu1rqwhQ4ZE+c9//nOi43vAukxrjWr7ppfqbvuH13cqVXS3/fvDH/5wlG1fefTRR6NsXThcqdZzh3junXrFq7DO6e02nd26qBm+LnYs+Pa3vx1ldv0ed9xxSTt2SVubvPDCC1H++c9/XvE88tiy2g1eaxm+1uPGjUt0F1xwQZR5zAXS6/T73/8+0X3zm9+M8uc+97lEx99xyimnRHnGjBkVz9F7ZtoxmI9v700eP6dPn57o2rMkTP33eCGEEEKIHGjyI4QQQohCocmPEEIIIQpFp8f8cMyF3SaAP2d9t56Pvlo/oecbrhQ3ZL+b45Lscew52jTuStjzryW/NdvEllbnFGQL+59feeWVRFftdgieXTmOgePBAOCSSy6J8nnnnZfoJk2aFGWbgtmzZ88o887XALB69eoo33rrrRXPq9bx+qYXL+eVZqj2fuVj2BgBjt3h7RGANDU7T+mCSu2AxogTqdaWixcvrvi5PLFQnM7Macje+OXt9u2VDfFKGuQ553osKWJ3Z/di8TgV3ZYm4NT3O++8M9EtXLgwyieeeGKUe/funbTjmEybzn7NNddE+eijj050fJ5//OMfE903vvGNKNsYvh49eqASrbWlVn6EEEIIUSg0+RFCCCFEoejwCs92mZrdB6eddlrFz3FqMZAuv3nL297O0bx8aM+LsenKvFR70kknVdTZXXV5adGmVTO1vBzL13r58uWJ7tVXX42y3fGX7WCXxXlJ1HNtsf1sWQR+zanQAPDEE09EecSIEYmOl3jZzQUAW7ZsifKqVasSHduWXTRAbae353EtsF1s/2hphV3+PranTXVnnXV78es8JSq4rbURt63l/sd4ZSisvfi1rbL89NNPR3nPPfdMdOzOuu666xLdgAEDosw7snOVYQD43e9+F2XrPj700EOjfMsttyS6X/7yl1FetmxZouPf6tmyXjn44IOjzNXkgXSctW6vadOmRdmOz3xf22fTb3/72yizW9lWeOYq/pxyDwC9evWKsr3/uBQJu7mAtK/byv1s27bul/V/lwghhBBC5ECTHyGEEEIUCk1+hBBCCFEo2iXmx/NFWzgt2aa5sd+Q/YnAe32RlT7H/kR7fBs3wrCvsVu3bonu9NNPj/Kpp56a6Nhnfu+99yY6jjmoV790tWUEvPgRL23aUsmWHGMApLa0x2Od3TV40aJFUbb3GMcn2JRLvifqJUakHHyt8sSQePb0YoD4OLwreFNTU9KOYxIefvjhRMf2tDE/HH/VpUuXRMevvViherWn1494C4FRo0YlugMPPDDKNuaH7WLjMdavXx/lBx98MMpLlixJ2q1bty7KHMsCpLvN8zY0AHDhhRdG2W6lw1thbNy4MdHx/eiVNKhlOBbR4sXbcbkRO0ZW2y85vtaWoGD72Vgr/j5b9oTjfGzsLY+7XvxkS0tqVKI+n8BCCCGEEC1Ekx8hhBBCFIoOSXXn5Sm7VMZLp7/5zW8SHe/sbJdq+Zhf//rXE12lneK9dHa73Mup6LwrMQBMnDgxytYdsmbNmihz+iDQvkt47YVXWTVPujAfxy7H8jKot7N4S23JLg5O3QVS+9kKz+zassf0bFlPLk3Pnl7aOF9/z57WtcyfO+igg6JsXdJchdhWRmf3lbUnu2msPblP57FnveDZcunSpVG2LpXBgwdH2dqB3Receg4As2bNijK7nry+yeMjAHz/+9+PMlcnBoCLLrooyvvuu2+iu/rqq6Ns3X382+2O9d/73veivGnTporn2dncd999Uba7unv36oc+9KEoc6kOAHj55ZejzCUGAOCMM86IMvcve3zPpcilXPj8AT+d3Rtn+Tvaelytn1FaCCGEEKIN0ORHCCGEEIVCkx8hhBBCFIp2ifnx4lm82JqpU6cmOvYBc/wPABx33HFRvuKKKxId77LNMQfWZ8i+TZtqO3bs2ChPmTIl0Q0cOLDsOQLAbbfdhkp4MRS1ij1P/g02LZFtaWMH+LVX0sCLH/F2n/ZsydtWWH8z67yYnzwpmPUEX0evb1odXw/Pnjatla+/jWVgePsaG9fDJQjsdglsT3sfNHoMF5+3vV9/+tOfRpm3mwDSfmW3PeBSEDZ9udq+ydfa65uPP/54ouPfMH78+EQ3dOjQsu2ANMZvn332SXQ33nhjlCdPnoxahUu5cFo/kG7RY58/o0ePjrJ9ZvK2Iza2ksddtqXtC7y1xrx58xIdx7jarYLYDjZO1htn2Jba3kIIIYQQohVo8iOEEEKIQtEhqe68dOVVELVp1TNnzoyyTc3r3bt3lDn1HEh3h9+6dWuUOd3TfjdXbQaAY445Jsq2wjMvz9qUPl5OtFVkvVTwenGd8G+wFXTZbeRVm7W/1XOl2eXZndj7iG1kl1yrdYXY31PtLuD1VEXWc+HYa8r29KrD5nGNspuKK7va7+7Xr1+U7Y7vvGxu7ckuMs9V1wg7gdv7zBtn2ZZcVR9I3SE2NZyPaa91tX2TbWKPwa+t7vnnn48yhzIAaVVqa0vetd5WlK7lvsnwb1q1alWi45CMPP2S73/rLmNbbt68OcqzZ89O2rFri91VANC/f/8oW9cW3wPeOGvHC++Z2Vrqr8cLIYQQQrQCTX6EEEIIUSg0+RFCCCFEoeiQVHcvndYryc6+QZsGzyXZL7jggkTXp0+fsse0W2Swj9mm0/J5Tps2LdHNnz+/4jmzz7Je43oYz5bWP8vX08Y78fW0MVScXmvTpiul0Fq/MR/TS1m3fmovDsSzV3v6ojuSlvZNvm72evN9YO253377RXnAgAEVv/uAAw6IMsf/AH78QEvjeuox5sfipbp7tvT6Jqdc2zIUXNLAu4/YXnacbWnMHcel2P7H21YsWLAA9Qj/piVLliS6Sy65JMrnnHNOouOxdNCgQYmOr+fatWsTHX8Hb2Nix/G+fftG2Yu3s+Ms3xPeONuR/bD+e7wQQgghRA40+RFCCCFEoejwXd296p92+ZJ1tlLsihUromx3ER42bFjZ7xs5cmTSjiud2h1wly1bFmVeYt3VOXvUs3tkJ54tvd3TWWeX1jn92e4CXqnaqF2O5WV+b/ncfq6lS66NYEuLlxrr6azbi0sL2HRoLlHBldKtXY466qiyMgDMnTs3yjY1u9FdWx7evey541ln+ya7uuw425K+6VXw9dyUjRBC0FLsb+Uxcs6cORV1XrV8q+PSJOxytu5Ttp9ny45MWW8pjT0aCCGEEEIYNPkRQgghRKHQ5EcIIYQQhaJDYn6qxfqKGS+OY926dYmOX7Ov0Zbq9rYoYP9znhToWvFndgbVbp3gpU3bcu3si/ZiGviYVuf5m71zLrItLXnigbhvWntyfBfHBtlj8I7vCxcuTHQ7duyo+DnvnIuEd+/a/sd4pQOsLSuVofD6n9dvvbge9cV/0h6xXdUen1/X+5Y/xR0dhBBCCFFINPkRQgghRKHocLdXnuUvXj63u4R7lYarpdplOrm5ymN/u7eTu+d64uVTe4xKbq88x6/UzlJkW5aDr5V1eXjuCs+enJrOlWTZzQUAK1eujPJdd92V6Fpq3yLjuTJY57m2Wkq1KetFTmdvKd79bp+L3vO0LcbZenNTaqQQQgghRKHQ5EcIIYQQhUKTHyGEEEIUik5Pda/WT2h9lF66ZntTD/7MzsDbOdrDixVqa2S7ltEeKeUzZ86M8owZM9r9+0QzXqxGnmvbkn6r/te+qG9Uj66UEEIIIQqFJj9CCCGEKBQhz9JlCOFlABva73TELjgwy7Leu262a2TLTqfNbAnInjWA+mbjIFs2FmXtmWvyI4QQQghR78jtJYQQQohCocmPEEIIIQqFJj9CCCGEKBQ1MfkJIZwVQlgdQlgbQrjWaXdzCOHUkjw/hHD8Lo67PoSwX47zmBxCuK2KdpeGENaU/l1K7z8SQuhV7fc1GiGEgSGEX4UQng0hPBNC+Fen7ZQQwiUleWoI4aO7OPYu7W3ajwghzK2i3d0hhM0hhJXm/W+FEEZV+32NSAjhsBDCMvr3eghhSoW2NWHPUtvdQghLuX0I4b9DCIOr/b5GQ32z8dBzs3V0+uQnhLAbgNsBjAFwJICPhxCOLNNuXwAnZln2eAefoj2PfQD8B4APAzgBwH+Q4e4FcGVnnVsN8A6Aq7MsOxLAiQD+VwVb7g7gUwB+1MHnV46pAM4q8/6tACoOKEUgy7LVWZYNybJsCID/AeBNAPfZdjVmTwD4VwDPmffuAHBNJ5xLraC+2UDoudl6On3yg+YLsTbLsnVZlv0NwH8DOLdMu/MB/KLcAUIId4QQnir9RfO/jfqaEMKKEMKiEMIhpfa9QwhzQgiLS/9OznG+ZwJ4OMuyrVmWvQrgYfyzgz4A4OM5jtVQZFn25yzLni7J29H8ABpQpukoAE9nWfaOVYQQvlyyycoQwvdDSErCXlxagVgZQjih1L576S/ERaW/9svdO945Pw5ga5n3NwDYN4TQL8/xGpjTAfyhdF0sNWPPEMIBAM4B8AOj+g2AM0oP98Khvtlw6LnZSmph8jMAwIv0+iWU75QnA1hS4RjXZ1l2PIBjAJwWQjiGdNuyLDsawG0Abi69dwuAm7IsG4bmm8MOlAghjA8h/Gee8y0ZtUtptl1oQgiDABwH4Hdl1J4tb8uybFiWZR8C0A3AWNLtVVqFuBLA3aX3rgfwWJZlJwAYCeD/hBC6m3M5PoTwHhtXwdOlcxXAhQBmVNDVkj1vRvMKz7v8ZpZl7wJYC+DYCp8rDOqbDYGem62knv4K6g/g5Qq6j4UQPo3m39MfzcuAy0u6GfT/TSX5DABH0h8uPUMITXzALMseQPOMNC+bAewPYEsLPtsQlK7lHABTsix7vUyT/nivW2InI0MI1wDYC8A+AJ4B8LOSbgbQ/BdhCKFnCOH9AEYDGB9C+PdSm64APsAHzLLsKQCXt+Cn7LRloQkh7AlgPIDrKjSpCXuGEMYC2Jxl2ZIQwogy57LTnpUeBg2P+mbh0HOzArUw+fkjgIH0+oDSe5a30Nx5EkIIBwH4dwDDsix7NYQw1bTLysjvQ7Mf9G1zrGrPd4Q53/n0umvpXAtJCGEPNA+u/y/Lsp9UaFbJll0BfBfA8VmWvRhC+Aoq23Ln6wDg/CzLVptj9W3ZL0gotC2JMWh2hfylgr5W7Hkymh+2Z5e+p2cI4YdZll1U0hfanuqbDYWem62kFtxeiwEMDiEcVPoL80KUnzk+B+CQMu/3BPAGgG2lTjXG6CfR/78tyb8EcNXOBiGEITnO9yEAo0MIvUJzwNbo0nso+cD7AVif43gNQ+n33wXguSzLvuM0rWTLnZ3vlUPVhMwAAAHbSURBVNJfFDbLZFLpe05B87LsNjRf+6t2xh+EEI5rxU+wHApg5S5bNT4fR2WXF1Aj9syy7Losyw7IsmwQmseRx2jiAxTYnuqbDYeem62k0yc/pcC6z6H5QjwHYFaWZc+Uafog0pnjzs//HsBSAKvQnKGwwDTpFUJYjuYMkH8rvfd5AMeHEJaHEJ4F8D/tcSv5LrMs2wrgv9B88y0G8J+l94DmjJgnywULFoSTAVwMYFT4Z3r02WXazQNwqn0zy7LXAPxfNA9qD6H5+jJvhxCWAvgegH8pvfdfAPYAsDyE8EzpdYIXVxBCmIHmzn1YCOGlEMK/lN7fA82DxlO7+M0NTSlG4yMAKq0UADVkT+d39AXwVpZlm/J8roFQ32wg9NxsPXW1t1cI4QkAY0sdseYIIdwC4IEsyx7t7HOpdUII9wG4JsuyNZ19LuUIIUwEMDTLsi919rnUA3Vgz38D8HqWZXd19rnUOnVgS/XNHOi5WZ5OX/nJydUwAXM1xkpNfKrmWjQH2dUquwP4dmefRB1R6/Z8DcC0zj6JOqHWbam+mQ89N8tQVys/QgghhBCtpd5WfoQQQgghWoUmP0IIIYQoFJr8CCGEEKJQaPIjhBBCiEKhyY8QQgghCsX/B+Ub2B5j+r4hAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 5 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwQsoGo-rYnm"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4lMSH5ZrbOp"
      },
      "source": [
        "def compute_acc_loss(forward_func, data_loader):\n",
        "    correct_cnt, ave_loss = 0, 0\n",
        "    for batch_idx, (x, target) in enumerate(data_loader):\n",
        "        with torch.no_grad():\n",
        "            target = target.cuda()\n",
        "            score, loss = forward_func(x.cuda(), target)\n",
        "            _, pred_label = torch.max(score.data, 1)\n",
        "            correct_cnt += (pred_label == target.data).sum().item()\n",
        "            ave_loss += loss.data.item() * len(x)\n",
        "    accuracy = correct_cnt * 1.0 / len(data_loader.dataset)\n",
        "    ave_loss /= len(data_loader.dataset)\n",
        "    return accuracy, ave_loss\n",
        "\n",
        "def train_test_acc_eval_f(net):\n",
        "    train_loader, _, test_loader = data_loader()\n",
        "    def forward_func(x, target):\n",
        "        y = net(x)\n",
        "        return y, net.loss(y, target)\n",
        "    with torch.no_grad():\n",
        "        acc_train, loss_train = compute_acc_loss(forward_func, train_loader)\n",
        "        acc_test, loss_test = compute_acc_loss(forward_func, test_loader)\n",
        "\n",
        "    print(f\"Train err: {100-acc_train*100:.2f}%, train loss: {loss_train}\")\n",
        "    print(f\"TEST ERR: {100-acc_test*100:.2f}%, test loss: {loss_test}\")\n",
        "  \n",
        "def train_val_acc_eval_f(net):\n",
        "    train_loader, val_loader, _ = data_loader()\n",
        "    def forward_func(x, target):\n",
        "        y = net(x)\n",
        "        return y, net.loss(y, target)\n",
        "    with torch.no_grad():\n",
        "        acc_train, loss_train = compute_acc_loss(forward_func, train_loader)\n",
        "        acc_val, loss_val = compute_acc_loss(forward_func, val_loader)\n",
        "\n",
        "    return acc_train, loss_train, acc_val, loss_val\n",
        "\n",
        "def visualize_errs(errs, subset, parameters):\n",
        "    epochs = np.arange(len(errs))\n",
        "    fig = plt.figure()\n",
        "    ax = plt.gca()\n",
        "    ax.plot(epochs, errs)\n",
        "    ax.set_xlabel('Epoch')                                                # When plotting error per epoch\n",
        "    ax.set_title(subset + ' Error per Epoch with ' + str(parameters))     # When plotting error per epoch\n",
        "    #ax.set_xlabel('SGD Step')                                            # When plotting error per SGD step\n",
        "    #ax.set_title(subset + ' Error per SGD step with ' + str(parameters)) # When plotting error per SGD step\n",
        "    ax.set_ylabel(subset + ' Error')\n",
        "    ax.set_xscale('linear')\n",
        "    ax.set_yscale('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhEPh4zCEvgO"
      },
      "source": [
        "##Reference Network\n",
        "We use cuda capable GPU for our experiments. The network has 3 fully-connected layers with dimensions 784x300, 300x100, and 100x5, and the total of 266105 parameters (which includes biases).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvFBm7snEnqe"
      },
      "source": [
        "device = torch.device('cuda') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKoCNKpN1y-U"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "        self.fc1 = nn.Linear(784, 300)\n",
        "        self.fc2 = nn.Linear(300, 100)\n",
        "        self.fc3 = nn.Linear(100, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_net(net, parameters):\n",
        "    train_errs = []\n",
        "    val_errs = []\n",
        "\n",
        "    max_val_acc = 0\n",
        "    #min_val_loss = np.inf\n",
        "    epochs_per_early_stop_check = parameters[\"epochs_per_early_stop_check\"]\n",
        "    early_stop_thresh = 1e-5\n",
        "    intitial_early_stop_patience = 3\n",
        "    early_stop_patience = intitial_early_stop_patience\n",
        "\n",
        "    train_loader, _, _ = data_loader(parameters[\"batch_size\"])\n",
        "    params = list(filter(lambda p: p.requires_grad, net.parameters()))\n",
        "    optimizer = optim.SGD(params, \n",
        "                          lr=parameters[\"lr\"], \n",
        "                          momentum=parameters[\"momentum\"], \n",
        "                          weight_decay=parameters[\"weight_decay\"],\n",
        "                          nesterov = parameters[\"nesterov\"])\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=parameters[\"step_size\"], gamma=parameters[\"gamma\"])\n",
        "    max_epochs=100\n",
        "    for epoch in range(max_epochs):\n",
        "        avg_loss = []\n",
        "        for x, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            x = x.cuda()\n",
        "            target = target.cuda().to(dtype=torch.long)\n",
        "            out = net(x)\n",
        "            loss = net.loss(out, target)\n",
        "            avg_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # -------------------------------------------------------------------------------\n",
        "\n",
        "            # ------------------------------------------------------------------------------- \n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"\\tepoch #{epoch} is finished.\")\n",
        "        print(f\"\\t  avg. train loss: {np.mean(avg_loss):.6f}\")\n",
        "        ## Note: when preparing final report, this chunk should be put in the inner loop\n",
        "        ## to record errors for each SGD step, rather than just for each epoch (visualize\n",
        "        ## errs function should also be modified accordingly)\n",
        "        ## During hyperparameter tuning, it will be sufficient to find error rates only\n",
        "        ## on each epoch\n",
        "        # ------------------------------------------------------------------------------- \n",
        "        acc_train, loss_train, acc_val, loss_val = train_val_acc_eval_f(net.eval())\n",
        "        print(f\"\\tTrain err: {100-acc_train*100:.2f}%, train loss: {loss_train}\")\n",
        "        print(f\"\\tValidation err: {100-acc_val*100:.5f}%, validation loss: {loss_val}\\n\")\n",
        "        train_errs.append(1 - acc_train)\n",
        "        val_errs.append(1 - acc_val)\n",
        "        # ------------------------------------------------------------------------------- \n",
        "        #if (min_val_loss - early_stop_thresh > loss_val):\n",
        "        if (epoch % epochs_per_early_stop_check == epochs_per_early_stop_check - 1):\n",
        "            if (max_val_acc + early_stop_thresh < acc_val):\n",
        "                #min_val_loss = loss_val\n",
        "                max_val_acc = acc_val\n",
        "                early_stop_patience = intitial_early_stop_patience\n",
        "            else:\n",
        "                early_stop_patience -= 1\n",
        "            if (early_stop_patience == 0):\n",
        "                break;\n",
        "    \n",
        "    visualize_errs(train_errs, \"Train\", parameters)\n",
        "    visualize_errs(val_errs, \"Validation\", parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv6adajZmlir"
      },
      "source": [
        "##Parameter Selection\n",
        "We use Stochastic Gradient Descent and try to find the optimal combination of momentum, batch size, learning rate, and l2 regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kV8Cjjmm7Tg"
      },
      "source": [
        "# Batch size: powers of 2 (2^0 (batch-size 1) to 2^16 (gradient descent)) # very small batch sizes take a long time for each epoch, very large batch sizes take longer to converge\n",
        "# likewise, patience too high for very small batch size\n",
        "# lr: small positive number to value at which train loss oscillates up and down from very start of training (perhaps try 1e-5 - 1e4)\n",
        "# gamma (factor by which learning rate decreases): small positive number to 1\n",
        "# step size (how many epochs pass between each lr scheduler step): integers from 1 up\n",
        "# momentum (addition of previous SGD step vector to current SGD step vector): 0 - 1 \n",
        "# weight decay (penalty to loss for magnitude of weights): 0 to very large number (train error doesn't decrease because CELoss term dwarfed by weight magnitude loss term)\n",
        "# nesterov (something to do with SGD, similar to momentum but looking at next update instead of previous): True or False\n",
        "\n",
        "# Notes: \n",
        "# Wanted to find a way so that we didn't have to train every model for a long time regardless of how fast it converged, but still be able to show that validation error either increases\n",
        "# or flattens out before giving up on a particular selection of hyperparameters\n",
        "#   Decided to set a maximum number of epochs and use early stopping with a threshold of 1e-5 on validation accuracy to determine when to stop\n",
        "#   Result is that (as long as the model didn't take longer than 100 epochs to converge) the validation accuracy curve will at least be flat for some time before stopping training\n",
        "#   introduced a new hyperparameter, epochs_per_early_stop_check, since some selections of hyperparameters will naturally take longer to converge than others\n",
        "#     Training loop will only check to see if validation accuracy hasn't improved every <epochs_per_early_stop_check> epochs, allowing us to give more time to models\n",
        "#     that naturally take longer to improve significantly\n",
        "#     If validation accuracy hasn't improved by at least 1e-5 (.001%) since its last check, a counter is decremented. If this occurs three times, the training loop breaks\n",
        "#\n",
        "# Started by seeing if I could find a smaller, reasonable range of batch sizes\n",
        "# for batch sizes 2^0, 2^1, 2^2 epochs each took a while and train error barely decreased, sometimes going up (lr may need to be lower in these cases)\n",
        "# for batch size 2^16 (gradient descent), early stopping settings were too stringent and the model wasn't given time to converge - patience should be higher for higher batch size\n",
        "# Changing learning rate to 0.01 on very small batch sizes worked very well, going to check progressively larger batch sizes to see if the learning rate is better in general\n",
        "#   Lower batch sizes make epochs take longer, but the model converges to a good accuracy in fewer epochs - initial run kept going for too long after min validation error reached\n",
        "#     so I reduced epochs per early stop check and trained again: validation error to beat 1.05144% (batch_size 8, lr: .01, epochs_per_early_stop_check: 3)\n",
        "#   batch size 16 not doing better on val set than batch size 8 with same lr - validation accuracy flattens\n",
        "#   batch sizes up to 256 weren't doing better than bs 8 with lr 0.01, so I tried increasing lr for larger batch sizes up to 0.05 and allowed model to train longer\n",
        "#     models trained in this way consistently reached a minimal train and validation error and almost completely stopped moving from that point on\n",
        "#     played around a bit with gamma and step size, but model error would consistently bottom and not oscillate, so I assumed the values were good as they were\n",
        "# Found that learning rates around 1 either don't work at all for smaller batch sizes, or oscillate a bit at the start then start behaving as lr_scheduler decreases it over time for larger \n",
        "#     batch sizes\n",
        "# For gradient descent (batch_size 2**16), learning rate starting at about 1 works well, but a learning rate as high as 2 doesn't even work well on a very large batch size\n",
        "#   Decreasing gamma helped with this, but it still underperforms compared to starting at a lower lr\n",
        "# Decided to set learning rate range to be small positive number to 2. \n",
        "# \n",
        "# Best results so far have been found by setting lr relatively low and having batch size very low. In effect we are taking a very large amount of small steps towards\n",
        "#   the minimizer, but I wonder if increasing the number of training epochs with a lower gamma and higher batch size may have the same effect but faster. Note: by the end of the first\n",
        "#   epoch with very small batch size, the training error is typically already around 1% and validation error less than 2%\n",
        "#   I have also noticed that training error will very quickly go down to 0%, and once there, the validation accuracy barely changes. \n",
        "#   I also wonder if decreasing the gamma a bit with small batch size and lr may allow us to reach a lower minimum, since I've seen the validation error reach a lower value but then\n",
        "#   increase and settle at a slightly higher value later in training\n",
        "# Tried decreasing gamma on lower batch size, best model is bs 4, lr .005, gamma .9, epochs per early stop check 2, with val accuracy of 0.94969%\n",
        "# Trying to get results comparable to low batch size with a batch size of 256, by varying lr, gamma, and step size\n",
        "#   While trying to do the above, we were kicked out of google colab and unable to get access to a GPU the same day\n",
        "#   When we went back the next day, we were unable to get the above results with as low of valudation error as we initially could, but we were able to get a model with a more\n",
        "#   reasonable batch size of 128 that had validation accuracy almost as low as the other one. Since we knew that we could get kicked out of google colab again with little time\n",
        "#   left to finish the project, we decided to stick with this as our final model\n",
        "# Models with larger batch sizes tended to end the first training epoch with higher validation error, likely since they have fewer SGD updates before we check the error,\n",
        "#   but always converged to a point of near-zero training error where the validation error was completely flat for many epochs. Models with smaller batch sizes took fewer\n",
        "#   epochs to train, and started out at lower error rates, but would generally not improve past the point that training error began to read 0% (which happened very quickly).\n",
        "parameters={\"batch_size\": 2**12, \"lr\": .1, \"gamma\":.9, \"step_size\": 1, \"momentum\": 0.9, \"weight_decay\": 0, \"nesterov\": True, \"epochs_per_early_stop_check\": 5}\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4EN6xeFUKNL6",
        "outputId": "7ed1290a-6714-49fd-c43c-2e1a209fcad8"
      },
      "source": [
        "train_net(net.to(device), parameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tepoch #0 is finished.\n",
            "\t  avg. train loss: 1.578813\n",
            "\tTrain err: 36.74%, train loss: 1.5037290783815607\n",
            "\tValidation err: 38.21368%, validation loss: 1.5057346959920035\n",
            "\n",
            "\tepoch #1 is finished.\n",
            "\t  avg. train loss: 1.371805\n",
            "\tTrain err: 27.42%, train loss: 1.0837939617245695\n",
            "\tValidation err: 28.62634%, validation loss: 1.09385119105814\n",
            "\n",
            "\tepoch #2 is finished.\n",
            "\t  avg. train loss: 0.847478\n",
            "\tTrain err: 15.16%, train loss: 0.5397541955922001\n",
            "\tValidation err: 15.90729%, validation loss: 0.5508656808385207\n",
            "\n",
            "\tepoch #3 is finished.\n",
            "\t  avg. train loss: 0.404762\n",
            "\tTrain err: 7.86%, train loss: 0.27925874431465947\n",
            "\tValidation err: 8.09497%, validation loss: 0.28042651806058283\n",
            "\n",
            "\tepoch #4 is finished.\n",
            "\t  avg. train loss: 0.251971\n",
            "\tTrain err: 6.57%, train loss: 0.21634927225667377\n",
            "\tValidation err: 6.59129%, validation loss: 0.21492562981868882\n",
            "\n",
            "\tepoch #5 is finished.\n",
            "\t  avg. train loss: 0.203169\n",
            "\tTrain err: 5.63%, train loss: 0.18561840651109238\n",
            "\tValidation err: 5.63030%, validation loss: 0.18199092200780873\n",
            "\n",
            "\tepoch #6 is finished.\n",
            "\t  avg. train loss: 0.168648\n",
            "\tTrain err: 5.07%, train loss: 0.16346894605446233\n",
            "\tValidation err: 4.96326%, validation loss: 0.15841258942508105\n",
            "\n",
            "\tepoch #7 is finished.\n",
            "\t  avg. train loss: 0.144265\n",
            "\tTrain err: 4.57%, train loss: 0.14734277237755383\n",
            "\tValidation err: 4.51102%, validation loss: 0.14170546304411374\n",
            "\n",
            "\tepoch #8 is finished.\n",
            "\t  avg. train loss: 0.135392\n",
            "\tTrain err: 4.22%, train loss: 0.136424656177676\n",
            "\tValidation err: 4.12663%, validation loss: 0.13141151218090172\n",
            "\n",
            "\tepoch #9 is finished.\n",
            "\t  avg. train loss: 0.126451\n",
            "\tTrain err: 4.03%, train loss: 0.12844846500444781\n",
            "\tValidation err: 3.93443%, validation loss: 0.12444044297330456\n",
            "\n",
            "\tepoch #10 is finished.\n",
            "\t  avg. train loss: 0.125507\n",
            "\tTrain err: 3.78%, train loss: 0.12194317996270897\n",
            "\tValidation err: 3.70831%, validation loss: 0.11854973544738871\n",
            "\n",
            "\tepoch #11 is finished.\n",
            "\t  avg. train loss: 0.112670\n",
            "\tTrain err: 3.63%, train loss: 0.11688005824883779\n",
            "\tValidation err: 3.60656%, validation loss: 0.11401159726441278\n",
            "\n",
            "\tepoch #12 is finished.\n",
            "\t  avg. train loss: 0.116542\n",
            "\tTrain err: 3.49%, train loss: 0.11282687519871912\n",
            "\tValidation err: 3.51611%, validation loss: 0.11058926578882793\n",
            "\n",
            "\tepoch #13 is finished.\n",
            "\t  avg. train loss: 0.109819\n",
            "\tTrain err: 3.37%, train loss: 0.10947522256494492\n",
            "\tValidation err: 3.35783%, validation loss: 0.10815509626038537\n",
            "\n",
            "\tepoch #14 is finished.\n",
            "\t  avg. train loss: 0.104879\n",
            "\tTrain err: 3.30%, train loss: 0.10692444633143817\n",
            "\tValidation err: 3.33522%, validation loss: 0.10640705083220472\n",
            "\n",
            "\tepoch #15 is finished.\n",
            "\t  avg. train loss: 0.107207\n",
            "\tTrain err: 3.24%, train loss: 0.10454334502303322\n",
            "\tValidation err: 3.30130%, validation loss: 0.10450109751076939\n",
            "\n",
            "\tepoch #16 is finished.\n",
            "\t  avg. train loss: 0.098777\n",
            "\tTrain err: 3.14%, train loss: 0.10225934024698051\n",
            "\tValidation err: 3.23347%, validation loss: 0.10245772993719598\n",
            "\n",
            "\tepoch #17 is finished.\n",
            "\t  avg. train loss: 0.097890\n",
            "\tTrain err: 3.12%, train loss: 0.10053456216588501\n",
            "\tValidation err: 3.16563%, validation loss: 0.1009099284274353\n",
            "\n",
            "\tepoch #18 is finished.\n",
            "\t  avg. train loss: 0.099699\n",
            "\tTrain err: 3.04%, train loss: 0.09910268102043358\n",
            "\tValidation err: 3.09780%, validation loss: 0.0997649366494085\n",
            "\n",
            "\tepoch #19 is finished.\n",
            "\t  avg. train loss: 0.110243\n",
            "\tTrain err: 2.98%, train loss: 0.09779047197842783\n",
            "\tValidation err: 3.10910%, validation loss: 0.09887569801510222\n",
            "\n",
            "\tepoch #20 is finished.\n",
            "\t  avg. train loss: 0.096018\n",
            "\tTrain err: 2.94%, train loss: 0.09657908860565156\n",
            "\tValidation err: 3.06388%, validation loss: 0.0981136843081987\n",
            "\n",
            "\tepoch #21 is finished.\n",
            "\t  avg. train loss: 0.114829\n",
            "\tTrain err: 2.87%, train loss: 0.09547728186891985\n",
            "\tValidation err: 3.00735%, validation loss: 0.09727014330920693\n",
            "\n",
            "\tepoch #22 is finished.\n",
            "\t  avg. train loss: 0.093300\n",
            "\tTrain err: 2.85%, train loss: 0.09457807577857676\n",
            "\tValidation err: 2.97343%, validation loss: 0.09646293027563375\n",
            "\n",
            "\tepoch #23 is finished.\n",
            "\t  avg. train loss: 0.106165\n",
            "\tTrain err: 2.81%, train loss: 0.09376251226247743\n",
            "\tValidation err: 2.97343%, validation loss: 0.09581357137320606\n",
            "\n",
            "\tepoch #24 is finished.\n",
            "\t  avg. train loss: 0.097506\n",
            "\tTrain err: 2.81%, train loss: 0.0930224772802619\n",
            "\tValidation err: 2.92821%, validation loss: 0.09526710689590097\n",
            "\n",
            "\tepoch #25 is finished.\n",
            "\t  avg. train loss: 0.091100\n",
            "\tTrain err: 2.79%, train loss: 0.09238527471473974\n",
            "\tValidation err: 2.89429%, validation loss: 0.09476138308871729\n",
            "\n",
            "\tepoch #26 is finished.\n",
            "\t  avg. train loss: 0.093396\n",
            "\tTrain err: 2.77%, train loss: 0.09182746561691743\n",
            "\tValidation err: 2.89429%, validation loss: 0.09432987347203099\n",
            "\n",
            "\tepoch #27 is finished.\n",
            "\t  avg. train loss: 0.090989\n",
            "\tTrain err: 2.75%, train loss: 0.09134729658217393\n",
            "\tValidation err: 2.88298%, validation loss: 0.09395425272584298\n",
            "\n",
            "\tepoch #28 is finished.\n",
            "\t  avg. train loss: 0.090478\n",
            "\tTrain err: 2.73%, train loss: 0.09091502538369607\n",
            "\tValidation err: 2.86037%, validation loss: 0.09363342315851463\n",
            "\n",
            "\tepoch #29 is finished.\n",
            "\t  avg. train loss: 0.091299\n",
            "\tTrain err: 2.71%, train loss: 0.09053104183701581\n",
            "\tValidation err: 2.87168%, validation loss: 0.0933474588741213\n",
            "\n",
            "\tepoch #30 is finished.\n",
            "\t  avg. train loss: 0.085999\n",
            "\tTrain err: 2.70%, train loss: 0.09018848190704981\n",
            "\tValidation err: 2.86037%, validation loss: 0.09307734095120308\n",
            "\n",
            "\tepoch #31 is finished.\n",
            "\t  avg. train loss: 0.093239\n",
            "\tTrain err: 2.70%, train loss: 0.0898664486385131\n",
            "\tValidation err: 2.84907%, validation loss: 0.09278993399602131\n",
            "\n",
            "\tepoch #32 is finished.\n",
            "\t  avg. train loss: 0.085680\n",
            "\tTrain err: 2.70%, train loss: 0.08957171477088632\n",
            "\tValidation err: 2.82646%, validation loss: 0.0925025222386055\n",
            "\n",
            "\tepoch #33 is finished.\n",
            "\t  avg. train loss: 0.090460\n",
            "\tTrain err: 2.71%, train loss: 0.08931874448014784\n",
            "\tValidation err: 2.81515%, validation loss: 0.09225551831715521\n",
            "\n",
            "\tepoch #34 is finished.\n",
            "\t  avg. train loss: 0.085868\n",
            "\tTrain err: 2.70%, train loss: 0.08909649463124977\n",
            "\tValidation err: 2.81515%, validation loss: 0.09206136549054869\n",
            "\n",
            "\tepoch #35 is finished.\n",
            "\t  avg. train loss: 0.084967\n",
            "\tTrain err: 2.69%, train loss: 0.08890117948600489\n",
            "\tValidation err: 2.79254%, validation loss: 0.09188603264184372\n",
            "\n",
            "\tepoch #36 is finished.\n",
            "\t  avg. train loss: 0.087223\n",
            "\tTrain err: 2.68%, train loss: 0.08872604146022205\n",
            "\tValidation err: 2.76993%, validation loss: 0.0917153353759163\n",
            "\n",
            "\tepoch #37 is finished.\n",
            "\t  avg. train loss: 0.090129\n",
            "\tTrain err: 2.66%, train loss: 0.08856864615466244\n",
            "\tValidation err: 2.76993%, validation loss: 0.0915583048662336\n",
            "\n",
            "\tepoch #38 is finished.\n",
            "\t  avg. train loss: 0.085450\n",
            "\tTrain err: 2.67%, train loss: 0.08842597146366918\n",
            "\tValidation err: 2.76993%, validation loss: 0.09142432243238403\n",
            "\n",
            "\tepoch #39 is finished.\n",
            "\t  avg. train loss: 0.082211\n",
            "\tTrain err: 2.66%, train loss: 0.08829946620750796\n",
            "\tValidation err: 2.75862%, validation loss: 0.09131053305063092\n",
            "\n",
            "\tepoch #40 is finished.\n",
            "\t  avg. train loss: 0.089077\n",
            "\tTrain err: 2.66%, train loss: 0.088181515742642\n",
            "\tValidation err: 2.73601%, validation loss: 0.09121420473163581\n",
            "\n",
            "\tepoch #41 is finished.\n",
            "\t  avg. train loss: 0.086650\n",
            "\tTrain err: 2.65%, train loss: 0.0880740712663924\n",
            "\tValidation err: 2.74731%, validation loss: 0.09113090535875361\n",
            "\n",
            "\tepoch #42 is finished.\n",
            "\t  avg. train loss: 0.083276\n",
            "\tTrain err: 2.65%, train loss: 0.08798403372598249\n",
            "\tValidation err: 2.74731%, validation loss: 0.09105247931587554\n",
            "\n",
            "\tepoch #43 is finished.\n",
            "\t  avg. train loss: 0.088848\n",
            "\tTrain err: 2.65%, train loss: 0.08790393279966457\n",
            "\tValidation err: 2.75862%, validation loss: 0.09098625660633354\n",
            "\n",
            "\tepoch #44 is finished.\n",
            "\t  avg. train loss: 0.087516\n",
            "\tTrain err: 2.65%, train loss: 0.08782996094042017\n",
            "\tValidation err: 2.75862%, validation loss: 0.09093352358781934\n",
            "\n",
            "\tepoch #45 is finished.\n",
            "\t  avg. train loss: 0.094311\n",
            "\tTrain err: 2.65%, train loss: 0.0877604057044946\n",
            "\tValidation err: 2.75862%, validation loss: 0.09088482995232733\n",
            "\n",
            "\tepoch #46 is finished.\n",
            "\t  avg. train loss: 0.081946\n",
            "\tTrain err: 2.65%, train loss: 0.08769531036301177\n",
            "\tValidation err: 2.73601%, validation loss: 0.09083888105344745\n",
            "\n",
            "\tepoch #47 is finished.\n",
            "\t  avg. train loss: 0.083003\n",
            "\tTrain err: 2.64%, train loss: 0.08763810373091882\n",
            "\tValidation err: 2.74731%, validation loss: 0.09079712225748632\n",
            "\n",
            "\tepoch #48 is finished.\n",
            "\t  avg. train loss: 0.086605\n",
            "\tTrain err: 2.64%, train loss: 0.08758735691392144\n",
            "\tValidation err: 2.75862%, validation loss: 0.09075649605254253\n",
            "\n",
            "\tepoch #49 is finished.\n",
            "\t  avg. train loss: 0.097675\n",
            "\tTrain err: 2.64%, train loss: 0.08754312256278918\n",
            "\tValidation err: 2.72470%, validation loss: 0.09071701689080508\n",
            "\n",
            "\tepoch #50 is finished.\n",
            "\t  avg. train loss: 0.086801\n",
            "\tTrain err: 2.64%, train loss: 0.08750158014685609\n",
            "\tValidation err: 2.72470%, validation loss: 0.09068212797828419\n",
            "\n",
            "\tepoch #51 is finished.\n",
            "\t  avg. train loss: 0.086739\n",
            "\tTrain err: 2.64%, train loss: 0.08746471443148547\n",
            "\tValidation err: 2.72470%, validation loss: 0.09065144306828977\n",
            "\n",
            "\tepoch #52 is finished.\n",
            "\t  avg. train loss: 0.086801\n",
            "\tTrain err: 2.64%, train loss: 0.08743219963570897\n",
            "\tValidation err: 2.72470%, validation loss: 0.090624779951377\n",
            "\n",
            "\tepoch #53 is finished.\n",
            "\t  avg. train loss: 0.092799\n",
            "\tTrain err: 2.64%, train loss: 0.08740269828212353\n",
            "\tValidation err: 2.72470%, validation loss: 0.09060080757900575\n",
            "\n",
            "\tepoch #54 is finished.\n",
            "\t  avg. train loss: 0.084962\n",
            "\tTrain err: 2.64%, train loss: 0.08737602051376372\n",
            "\tValidation err: 2.72470%, validation loss: 0.09057814432460753\n",
            "\n",
            "\tepoch #55 is finished.\n",
            "\t  avg. train loss: 0.083149\n",
            "\tTrain err: 2.64%, train loss: 0.08735219809667084\n",
            "\tValidation err: 2.72470%, validation loss: 0.09056046919390737\n",
            "\n",
            "\tepoch #56 is finished.\n",
            "\t  avg. train loss: 0.084708\n",
            "\tTrain err: 2.64%, train loss: 0.08733086814714032\n",
            "\tValidation err: 2.72470%, validation loss: 0.09054438008838889\n",
            "\n",
            "\tepoch #57 is finished.\n",
            "\t  avg. train loss: 0.095258\n",
            "\tTrain err: 2.64%, train loss: 0.08731207280426986\n",
            "\tValidation err: 2.72470%, validation loss: 0.09052764688291544\n",
            "\n",
            "\tepoch #58 is finished.\n",
            "\t  avg. train loss: 0.088875\n",
            "\tTrain err: 2.64%, train loss: 0.08729406234829924\n",
            "\tValidation err: 2.72470%, validation loss: 0.09051226980272425\n",
            "\n",
            "\tepoch #59 is finished.\n",
            "\t  avg. train loss: 0.085852\n",
            "\tTrain err: 2.64%, train loss: 0.08727787771197253\n",
            "\tValidation err: 2.72470%, validation loss: 0.09050135792582098\n",
            "\n",
            "\tepoch #60 is finished.\n",
            "\t  avg. train loss: 0.091374\n",
            "\tTrain err: 2.64%, train loss: 0.08726366409497667\n",
            "\tValidation err: 2.72470%, validation loss: 0.09049284576173554\n",
            "\n",
            "\tepoch #61 is finished.\n",
            "\t  avg. train loss: 0.087593\n",
            "\tTrain err: 2.64%, train loss: 0.08725088318420011\n",
            "\tValidation err: 2.72470%, validation loss: 0.0904842786427741\n",
            "\n",
            "\tepoch #62 is finished.\n",
            "\t  avg. train loss: 0.093779\n",
            "\tTrain err: 2.64%, train loss: 0.0872391719573228\n",
            "\tValidation err: 2.72470%, validation loss: 0.0904756111907312\n",
            "\n",
            "\tepoch #63 is finished.\n",
            "\t  avg. train loss: 0.080059\n",
            "\tTrain err: 2.64%, train loss: 0.08722867568911508\n",
            "\tValidation err: 2.72470%, validation loss: 0.09046774660044972\n",
            "\n",
            "\tepoch #64 is finished.\n",
            "\t  avg. train loss: 0.079066\n",
            "\tTrain err: 2.64%, train loss: 0.08721974629533383\n",
            "\tValidation err: 2.72470%, validation loss: 0.09046053656283713\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gar0dZG0a2i9"
      },
      "source": [
        "PATH = \"/content/state_dicts/\" + str(parameters) + \".pt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DSUq1zzagnT"
      },
      "source": [
        "torch.save(net.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g28SqTpRaqER",
        "outputId": "065551bf-08ca-42b2-e777-327e7cdaae21"
      },
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5VM-Oqff6qO",
        "outputId": "679250e0-4ba8-4571-eef1-fcb3d9072634"
      },
      "source": [
        "parameters={\"batch_size\": 2**8, \"lr\": 0.2, \"gamma\":0.9, \"step_size\": 2, \"momentum\": 0.9, \"weight_decay\": 0, \"nesterov\": True}\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load(\"/content/state_dicts/\" + str(parameters) + \".pt\"))\n",
        "acc_train, loss_train, acc_val, loss_val = train_val_acc_eval_f(net.cuda().eval())\n",
        "print(f\"\\tTrain err: {100-acc_train*100:.5f}%, train loss: {loss_train}\")\n",
        "print(f\"\\tValidation err: {100-acc_val*100:.5f}%, validation loss: {loss_val}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain err: 0.00000%, train loss: 0.0001996844643708896\n",
            "\tValidation err: 0.97230%, validation loss: 0.042836139972783405\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNnIHErPYjKo",
        "outputId": "ec380a96-e1be-445f-a089-6c75ea79fb01"
      },
      "source": [
        "train_test_acc_eval_f(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train err: 0.00%, train loss: 0.00019968445818914577\n",
            "TEST ERR: 1.33%, test loss: 0.05709494277987256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRouY3-VFJLB"
      },
      "source": [
        "## Compression using the LC toolkit\n",
        "### Step 1: L step\n",
        "We will use same L step with same hyperparamters for all our compression examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3IMWCA8Gttp"
      },
      "source": [
        "# l_step_parameters: keep parameters \n",
        "#   learning rate: 0.7*(0.98**step), \n",
        "#   epochs per step: 7, \n",
        "\n",
        "# quantization parameters: codebook size (k) - powers of 2\n",
        "# low rank parameters: alpha - higher alpha gives greater compression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG5iJunyFB4j"
      },
      "source": [
        "def my_l_step(model, lc_penalty, step):\n",
        "    train_loader, val_loader, test_loader = data_loader()\n",
        "    params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    # ------------------- Learning rate parameter\n",
        "    lr = 0.7*(0.98**step)\n",
        "    # -------------------------------------------\n",
        "    optimizer = optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\n",
        "    print(f'L-step #{step} with lr: {lr:.5f}')\n",
        "    epochs_per_step_ = 7\n",
        "    if step == 0:\n",
        "        epochs_per_step_ = epochs_per_step_ * 2\n",
        "    for epoch in range(epochs_per_step_):\n",
        "        avg_loss = []\n",
        "        for x, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            x = x.to(device)\n",
        "            target = target.to(dtype=torch.long, device=device)\n",
        "            out = model(x)\n",
        "            loss = model.loss(out, target) + lc_penalty()\n",
        "            avg_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"\\tepoch #{epoch} is finished.\")\n",
        "        print(f\"\\t  avg. train loss: {np.mean(avg_loss):.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxj7fpehFNAX"
      },
      "source": [
        "### Step 2: Schedule of mu values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmgVCpF_FLKc"
      },
      "source": [
        "# mu schedule parameters: mu value should start very low and increase\n",
        "\n",
        "mu_s = [9e-5 * (1.1 ** n) for n in range(20)] # 0 to infinity\n",
        "# 20 L-C steps in total\n",
        "# total training epochs is 7 x 20 = 140"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQuLZemkIRXZ"
      },
      "source": [
        "### Compression time! Pruning\n",
        "Let us prune all but 5% of the weights in the network (5% = 13310 weights)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXalSNwdFOcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c6d894-005a-4ce5-852f-6bba7648b920"
      },
      "source": [
        "parameters={\"batch_size\": 2**8, \"lr\": 0.2, \"gamma\":0.9, \"step_size\": 2, \"momentum\": 0.9, \"weight_decay\": 0, \"nesterov\": True, \"epochs_per_early_stop_check\": 3}\n",
        "net = Net().cuda()\n",
        "net.load_state_dict(torch.load(\"/content/state_dicts/\" + str(parameters) + \".pt\"))\n",
        "\n",
        "# pruning parameter: kappa - controls how many weights will be in the compressed model (excluding biases)\n",
        "#   model starting size (with biases): 266105\n",
        "#   Starting number of weights: 265700\n",
        "#   30x compression: kappa = 6700\n",
        "#   5x compression: kappa = 46250\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "compression_tasks = {\n",
        "    Param(layers, device): (AsVector, ConstraintL0Pruning(\n",
        "            kappa=46250\n",
        "          ), 'pruning')\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()                              # entry point to the LC algorithm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from retrieve 276.9761156987101\n",
            "L0 constrained pruning.\n",
            "l0-cons pruning finished. #zeros: 82.59%\n",
            "Direct compression has been performed.\n",
            "Train err: 0.00%, train loss: 0.0021809499061038328\n",
            "TEST ERR: 1.39%, test loss: 0.05007311457078637\n",
            "9e-05\n",
            "L-step #0 with lr: 0.70000\n",
            "\tepoch #0 is finished.\n",
            "\t  avg. train loss: 0.004144\n",
            "\tepoch #1 is finished.\n",
            "\t  avg. train loss: 0.004109\n",
            "from retrieve 276.05937899055033\n",
            "L-step #0 has finished.\n",
            "L0 constrained pruning.\n",
            "l0-cons pruning finished. #zeros: 82.59%\n",
            "C-step #0 has finished.\n",
            "Lagrange multipliers have been updated.\n",
            "Train err: 0.00%, train loss: 0.0020892680660539935\n",
            "TEST ERR: 1.39%, test loss: 0.05023994376932429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVSPVNhhamWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6fc83e-f4ef-462c-e664-c1993e77501e"
      },
      "source": [
        "lc_alg.count_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46250"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqxtsfdwapMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d185cf4-3549-4232-a9fa-21658c78ecef"
      },
      "source": [
        "compressed_model_bits = lc_alg.count_param_bits() + (300+100+5)*32\n",
        "uncompressed_model_bits = (784*300+300*100+100*5 + 300 + 100 + 5)*32\n",
        "compression_ratio = uncompressed_model_bits/compressed_model_bits\n",
        "print(compression_ratio)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.003181571678783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a0txepca0vz"
      },
      "source": [
        "Note that we were pruning 95% of the weights. Naively, you would assume 20x compression ratio (100%/5%), however, this is not the case. Firstly, there are some uncompressed parts (in this case biases), and, secondly, storing a compressed model requires additional metadata (in this case positions of non-zero elements). Therefore we get only 16x compression ratio (vs naively expected 20x). \n",
        "\n",
        "To prevent manual computation of compression ratio, let us create a function below. Note, this function is model specific."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzgslnqLa4Bs"
      },
      "source": [
        "def compute_compression_ratio(lc_alg):\n",
        "    compressed_model_bits = lc_alg.count_param_bits() + (300+100+10)*32\n",
        "    uncompressed_model_bits = (784*300+300*100+100*10 + 300 + 100 + 10)*32\n",
        "    compression_ratio = uncompressed_model_bits/compressed_model_bits\n",
        "    return compression_ratio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tT9Mdm-IT9w"
      },
      "source": [
        "### Quantization\n",
        "Now let us quantize each layer with its own codebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndXQzWxcFQUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dde681d-652a-43a1-ff6d-0e14289e2c15"
      },
      "source": [
        "parameters={\"batch_size\": 2**8, \"lr\": 0.2, \"gamma\":0.9, \"step_size\": 2, \"momentum\": 0.9, \"weight_decay\": 0, \"nesterov\": True, \"epochs_per_early_stop_check\": 3}\n",
        "net = Net().cuda()\n",
        "net.load_state_dict(torch.load(\"/content/state_dicts/\" + str(parameters) + \".pt\"))\n",
        "\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "# k = 2 for each layer gives 30x compression\n",
        "# k = 4 for each layer gives 15x compression\n",
        "# k = 8 for each layer gives 10x compression\n",
        "# k = 16 for each layer gives about 8x compression\n",
        "# k = 32 for each layer gives about 6.2x compression\n",
        "# k = 64 for each layer gives about 5.28x compresssion\n",
        "# k = 128 for each layer gives about 4.5x compression\n",
        "\n",
        "compression_tasks = {\n",
        "    Param(layers[0], device): (AsVector, AdaptiveQuantization(k=128), 'layer0_quant'),\n",
        "    Param(layers[1], device): (AsVector, AdaptiveQuantization(k=128), 'layer1_quant'),\n",
        "    Param(layers[2], device): (AsVector, AdaptiveQuantization(k=128), 'layer2_quant')\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()  \n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from retrieve 174.84155325678307\n",
            "from retrieve 69.29116793712554\n",
            "from retrieve 32.8433945048008\n",
            "None k-means++\n",
            "K-Means converged in 300 iterations.\n",
            "None k-means++\n",
            "K-Means converged in 39 iterations.\n",
            "None k-means++\n",
            "K-Means converged in 4 iterations.\n",
            "Direct compression has been performed.\n",
            "Train err: 0.00%, train loss: 0.0002007996704738024\n",
            "TEST ERR: 1.31%, test loss: 0.05693264790107867\n",
            "9e-05\n",
            "L-step #0 with lr: 0.70000\n",
            "\tepoch #0 is finished.\n",
            "\t  avg. train loss: 0.000197\n",
            "\tepoch #1 is finished.\n",
            "\t  avg. train loss: 0.000204\n",
            "from retrieve 175.0665561670807\n",
            "from retrieve 69.5178068935247\n",
            "from retrieve 33.07397414575733\n",
            "L-step #0 has finished.\n",
            "{'cluster_centers': array([[ 0.08340073],\n",
            "       [-0.00807121],\n",
            "       [ 0.02971549],\n",
            "       [-0.03673408],\n",
            "       [ 0.01219443],\n",
            "       [-0.08436   ],\n",
            "       [-0.02212693],\n",
            "       [ 0.05087376],\n",
            "       [ 0.00262428],\n",
            "       [-0.01590441],\n",
            "       [-0.04851483],\n",
            "       [ 0.019622  ],\n",
            "       [-0.06808996],\n",
            "       [ 0.0353951 ],\n",
            "       [-0.02744987],\n",
            "       [-0.16972218],\n",
            "       [ 0.06201722],\n",
            "       [ 0.02632575],\n",
            "       [-0.00361312],\n",
            "       [ 0.00900968],\n",
            "       [ 0.10528208],\n",
            "       [ 0.04330106],\n",
            "       [-0.11743644],\n",
            "       [-0.0331056 ],\n",
            "       [-0.04126806],\n",
            "       [-0.05256189],\n",
            "       [-0.00048969],\n",
            "       [-0.01146122],\n",
            "       [ 0.02291121],\n",
            "       [ 0.01431972],\n",
            "       [ 0.07044414],\n",
            "       [-0.09440272],\n",
            "       [-0.01898164],\n",
            "       [-0.07522876],\n",
            "       [-0.03085665],\n",
            "       [ 0.05979657],\n",
            "       [ 0.1243253 ],\n",
            "       [ 0.039902  ],\n",
            "       [ 0.03306585],\n",
            "       [ 0.0057845 ],\n",
            "       [-0.04290908],\n",
            "       [-0.00581183],\n",
            "       [-0.0595595 ],\n",
            "       [-0.02420124],\n",
            "       [ 0.01749697],\n",
            "       [ 0.05296435],\n",
            "       [-0.01485163],\n",
            "       [ 0.02405619],\n",
            "       [ 0.01114887],\n",
            "       [ 0.07698287],\n",
            "       [ 0.04694118],\n",
            "       [-0.01031122],\n",
            "       [ 0.15809846],\n",
            "       [ 0.03192524],\n",
            "       [-0.11016022],\n",
            "       [ 0.09454337],\n",
            "       [ 0.02177681],\n",
            "       [-0.19491148],\n",
            "       [-0.06503092],\n",
            "       [-0.12601993],\n",
            "       [-0.0015206 ],\n",
            "       [-0.02634892],\n",
            "       [-0.07143867],\n",
            "       [-0.01694514],\n",
            "       [-0.0137405 ],\n",
            "       [ 0.00053331],\n",
            "       [ 0.03683016],\n",
            "       [-0.02854538],\n",
            "       [ 0.02518494],\n",
            "       [-0.13639481],\n",
            "       [-0.04468375],\n",
            "       [ 0.00684851],\n",
            "       [-0.0353623 ],\n",
            "       [-0.03972956],\n",
            "       [ 0.05517532],\n",
            "       [ 0.0800951 ],\n",
            "       [ 0.01326121],\n",
            "       [-0.00919599],\n",
            "       [ 0.00366292],\n",
            "       [ 0.01007308],\n",
            "       [-0.02002361],\n",
            "       [-0.00470421],\n",
            "       [-0.05478729],\n",
            "       [ 0.06450527],\n",
            "       [ 0.02747053],\n",
            "       [-0.10427361],\n",
            "       [ 0.0736791 ],\n",
            "       [-0.24178328],\n",
            "       [ 0.11268371],\n",
            "       [ 0.03080493],\n",
            "       [-0.08935295],\n",
            "       [-0.05055796],\n",
            "       [-0.07955895],\n",
            "       [-0.00255426],\n",
            "       [ 0.01855373],\n",
            "       [ 0.01644567],\n",
            "       [-0.0231639 ],\n",
            "       [ 0.06737353],\n",
            "       [-0.00694456],\n",
            "       [ 0.08688121],\n",
            "       [ 0.03422099],\n",
            "       [ 0.03836856],\n",
            "       [-0.03421866],\n",
            "       [-0.03198536],\n",
            "       [-0.01796812],\n",
            "       [ 0.04510592],\n",
            "       [-0.03820328],\n",
            "       [ 0.05749939],\n",
            "       [-0.06221319],\n",
            "       [ 0.04152793],\n",
            "       [-0.04656196],\n",
            "       [ 0.02860317],\n",
            "       [-0.02108147],\n",
            "       [ 0.13769799],\n",
            "       [-0.15146456],\n",
            "       [-0.05713124],\n",
            "       [-0.02968722],\n",
            "       [ 0.00793101],\n",
            "       [-0.09870395],\n",
            "       [ 0.09973088],\n",
            "       [ 0.02070582],\n",
            "       [-0.01261645],\n",
            "       [ 0.00472091],\n",
            "       [ 0.00158171],\n",
            "       [ 0.04884272],\n",
            "       [-0.02524562],\n",
            "       [ 0.09053533],\n",
            "       [ 0.01537243]]), 'assignments': array([122,  84,  43, ..., 125,   1,  53], dtype=int32), 'data_shape': (235200,)} [[ 0.08340073]\n",
            " [-0.00807121]\n",
            " [ 0.02971549]\n",
            " [-0.03673408]\n",
            " [ 0.01219443]\n",
            " [-0.08436   ]\n",
            " [-0.02212693]\n",
            " [ 0.05087376]\n",
            " [ 0.00262428]\n",
            " [-0.01590441]\n",
            " [-0.04851483]\n",
            " [ 0.019622  ]\n",
            " [-0.06808996]\n",
            " [ 0.0353951 ]\n",
            " [-0.02744987]\n",
            " [-0.16972218]\n",
            " [ 0.06201722]\n",
            " [ 0.02632575]\n",
            " [-0.00361312]\n",
            " [ 0.00900968]\n",
            " [ 0.10528208]\n",
            " [ 0.04330106]\n",
            " [-0.11743644]\n",
            " [-0.0331056 ]\n",
            " [-0.04126806]\n",
            " [-0.05256189]\n",
            " [-0.00048969]\n",
            " [-0.01146122]\n",
            " [ 0.02291121]\n",
            " [ 0.01431972]\n",
            " [ 0.07044414]\n",
            " [-0.09440272]\n",
            " [-0.01898164]\n",
            " [-0.07522876]\n",
            " [-0.03085665]\n",
            " [ 0.05979657]\n",
            " [ 0.1243253 ]\n",
            " [ 0.039902  ]\n",
            " [ 0.03306585]\n",
            " [ 0.0057845 ]\n",
            " [-0.04290908]\n",
            " [-0.00581183]\n",
            " [-0.0595595 ]\n",
            " [-0.02420124]\n",
            " [ 0.01749697]\n",
            " [ 0.05296435]\n",
            " [-0.01485163]\n",
            " [ 0.02405619]\n",
            " [ 0.01114887]\n",
            " [ 0.07698287]\n",
            " [ 0.04694118]\n",
            " [-0.01031122]\n",
            " [ 0.15809846]\n",
            " [ 0.03192524]\n",
            " [-0.11016022]\n",
            " [ 0.09454337]\n",
            " [ 0.02177681]\n",
            " [-0.19491148]\n",
            " [-0.06503092]\n",
            " [-0.12601993]\n",
            " [-0.0015206 ]\n",
            " [-0.02634892]\n",
            " [-0.07143867]\n",
            " [-0.01694514]\n",
            " [-0.0137405 ]\n",
            " [ 0.00053331]\n",
            " [ 0.03683016]\n",
            " [-0.02854538]\n",
            " [ 0.02518494]\n",
            " [-0.13639481]\n",
            " [-0.04468375]\n",
            " [ 0.00684851]\n",
            " [-0.0353623 ]\n",
            " [-0.03972956]\n",
            " [ 0.05517532]\n",
            " [ 0.0800951 ]\n",
            " [ 0.01326121]\n",
            " [-0.00919599]\n",
            " [ 0.00366292]\n",
            " [ 0.01007308]\n",
            " [-0.02002361]\n",
            " [-0.00470421]\n",
            " [-0.05478729]\n",
            " [ 0.06450527]\n",
            " [ 0.02747053]\n",
            " [-0.10427361]\n",
            " [ 0.0736791 ]\n",
            " [-0.24178328]\n",
            " [ 0.11268371]\n",
            " [ 0.03080493]\n",
            " [-0.08935295]\n",
            " [-0.05055796]\n",
            " [-0.07955895]\n",
            " [-0.00255426]\n",
            " [ 0.01855373]\n",
            " [ 0.01644567]\n",
            " [-0.0231639 ]\n",
            " [ 0.06737353]\n",
            " [-0.00694456]\n",
            " [ 0.08688121]\n",
            " [ 0.03422099]\n",
            " [ 0.03836856]\n",
            " [-0.03421866]\n",
            " [-0.03198536]\n",
            " [-0.01796812]\n",
            " [ 0.04510592]\n",
            " [-0.03820328]\n",
            " [ 0.05749939]\n",
            " [-0.06221319]\n",
            " [ 0.04152793]\n",
            " [-0.04656196]\n",
            " [ 0.02860317]\n",
            " [-0.02108147]\n",
            " [ 0.13769799]\n",
            " [-0.15146456]\n",
            " [-0.05713124]\n",
            " [-0.02968722]\n",
            " [ 0.00793101]\n",
            " [-0.09870395]\n",
            " [ 0.09973088]\n",
            " [ 0.02070582]\n",
            " [-0.01261645]\n",
            " [ 0.00472091]\n",
            " [ 0.00158171]\n",
            " [ 0.04884272]\n",
            " [-0.02524562]\n",
            " [ 0.09053533]\n",
            " [ 0.01537243]]\n",
            "K-Means converged in 19 iterations.\n",
            "{'cluster_centers': array([[-0.01563013],\n",
            "       [ 0.07036636],\n",
            "       [-0.0576254 ],\n",
            "       [ 0.02297883],\n",
            "       [ 0.13856096],\n",
            "       [ 0.04988637],\n",
            "       [-0.09252773],\n",
            "       [-0.03547273],\n",
            "       [ 0.10630776],\n",
            "       [ 0.00283005],\n",
            "       [-0.12194856],\n",
            "       [ 0.03538387],\n",
            "       [ 0.18151555],\n",
            "       [ 0.01305837],\n",
            "       [-0.02642901],\n",
            "       [ 0.08557792],\n",
            "       [-0.04471583],\n",
            "       [-0.06977439],\n",
            "       [-0.0062458 ],\n",
            "       [ 0.06053427],\n",
            "       [-0.15471933],\n",
            "       [ 0.04116635],\n",
            "       [ 0.24212021],\n",
            "       [ 0.09660909],\n",
            "       [ 0.009718  ],\n",
            "       [ 0.02982181],\n",
            "       [ 0.11980486],\n",
            "       [-0.08193521],\n",
            "       [-0.05198886],\n",
            "       [ 0.1513731 ],\n",
            "       [-0.10686807],\n",
            "       [-0.02129698],\n",
            "       [ 0.07749971],\n",
            "       [-0.0319764 ],\n",
            "       [-0.00074651],\n",
            "       [ 0.01947398],\n",
            "       [ 0.05311763],\n",
            "       [ 0.04479533],\n",
            "       [-0.00969206],\n",
            "       [-0.06474702],\n",
            "       [-0.0410961 ],\n",
            "       [-0.07511369],\n",
            "       [-0.17760507],\n",
            "       [ 0.05491056],\n",
            "       [ 0.06627591],\n",
            "       [ 0.21714868],\n",
            "       [ 0.09095572],\n",
            "       [ 0.03732377],\n",
            "       [ 0.00623868],\n",
            "       [ 0.12957455],\n",
            "       [ 0.02641121],\n",
            "       [-0.00443068],\n",
            "       [-0.01952861],\n",
            "       [-0.13633622],\n",
            "       [ 0.03157526],\n",
            "       [ 0.16494688],\n",
            "       [-0.04828021],\n",
            "       [ 0.26754329],\n",
            "       [ 0.01616634],\n",
            "       [-0.10151408],\n",
            "       [ 0.06431855],\n",
            "       [-0.03009683],\n",
            "       [-0.05994099],\n",
            "       [-0.03738506],\n",
            "       [-0.01359611],\n",
            "       [ 0.0995118 ],\n",
            "       [ 0.04652242],\n",
            "       [ 0.08257809],\n",
            "       [-0.02298774],\n",
            "       [ 0.19228337],\n",
            "       [-0.05378803],\n",
            "       [ 0.11479326],\n",
            "       [-0.08572732],\n",
            "       [ 0.07512266],\n",
            "       [-0.07241275],\n",
            "       [-0.11239339],\n",
            "       [-0.01764989],\n",
            "       [-0.00264561],\n",
            "       [ 0.14428186],\n",
            "       [ 0.01140611],\n",
            "       [ 0.03925062],\n",
            "       [ 0.00455833],\n",
            "       [-0.14535521],\n",
            "       [ 0.02122581],\n",
            "       [-0.0336757 ],\n",
            "       [-0.09698093],\n",
            "       [-0.0465395 ],\n",
            "       [-0.05564685],\n",
            "       [ 0.15774633],\n",
            "       [ 0.1100466 ],\n",
            "       [-0.1940692 ],\n",
            "       [-0.16462241],\n",
            "       [-0.04296012],\n",
            "       [ 0.05859482],\n",
            "       [ 0.1025913 ],\n",
            "       [-0.12813982],\n",
            "       [ 0.03343733],\n",
            "       [ 0.0937581 ],\n",
            "       [-0.07843308],\n",
            "       [ 0.17370357],\n",
            "       [ 0.12467393],\n",
            "       [-0.02832507],\n",
            "       [ 0.05147001],\n",
            "       [ 0.079885  ],\n",
            "       [ 0.00098249],\n",
            "       [ 0.04305971],\n",
            "       [ 0.06827873],\n",
            "       [ 0.00800575],\n",
            "       [ 0.06249518],\n",
            "       [ 0.08851393],\n",
            "       [-0.01156387],\n",
            "       [ 0.017748  ],\n",
            "       [-0.02462326],\n",
            "       [ 0.04817148],\n",
            "       [ 0.02478664],\n",
            "       [-0.05018728],\n",
            "       [-0.11651197],\n",
            "       [-0.06241781],\n",
            "       [ 0.13388806],\n",
            "       [ 0.02810952],\n",
            "       [ 0.01460383],\n",
            "       [-0.03925389],\n",
            "       [-0.06740711],\n",
            "       [-0.08962184],\n",
            "       [ 0.07272945],\n",
            "       [-0.00799549],\n",
            "       [ 0.2041016 ],\n",
            "       [ 0.05669838]]), 'assignments': array([  4,  14, 105, ...,  64,   0, 119], dtype=int32), 'data_shape': (30000,)} [[-0.01563013]\n",
            " [ 0.07036636]\n",
            " [-0.0576254 ]\n",
            " [ 0.02297883]\n",
            " [ 0.13856096]\n",
            " [ 0.04988637]\n",
            " [-0.09252773]\n",
            " [-0.03547273]\n",
            " [ 0.10630776]\n",
            " [ 0.00283005]\n",
            " [-0.12194856]\n",
            " [ 0.03538387]\n",
            " [ 0.18151555]\n",
            " [ 0.01305837]\n",
            " [-0.02642901]\n",
            " [ 0.08557792]\n",
            " [-0.04471583]\n",
            " [-0.06977439]\n",
            " [-0.0062458 ]\n",
            " [ 0.06053427]\n",
            " [-0.15471933]\n",
            " [ 0.04116635]\n",
            " [ 0.24212021]\n",
            " [ 0.09660909]\n",
            " [ 0.009718  ]\n",
            " [ 0.02982181]\n",
            " [ 0.11980486]\n",
            " [-0.08193521]\n",
            " [-0.05198886]\n",
            " [ 0.1513731 ]\n",
            " [-0.10686807]\n",
            " [-0.02129698]\n",
            " [ 0.07749971]\n",
            " [-0.0319764 ]\n",
            " [-0.00074651]\n",
            " [ 0.01947398]\n",
            " [ 0.05311763]\n",
            " [ 0.04479533]\n",
            " [-0.00969206]\n",
            " [-0.06474702]\n",
            " [-0.0410961 ]\n",
            " [-0.07511369]\n",
            " [-0.17760507]\n",
            " [ 0.05491056]\n",
            " [ 0.06627591]\n",
            " [ 0.21714868]\n",
            " [ 0.09095572]\n",
            " [ 0.03732377]\n",
            " [ 0.00623868]\n",
            " [ 0.12957455]\n",
            " [ 0.02641121]\n",
            " [-0.00443068]\n",
            " [-0.01952861]\n",
            " [-0.13633622]\n",
            " [ 0.03157526]\n",
            " [ 0.16494688]\n",
            " [-0.04828021]\n",
            " [ 0.26754329]\n",
            " [ 0.01616634]\n",
            " [-0.10151408]\n",
            " [ 0.06431855]\n",
            " [-0.03009683]\n",
            " [-0.05994099]\n",
            " [-0.03738506]\n",
            " [-0.01359611]\n",
            " [ 0.0995118 ]\n",
            " [ 0.04652242]\n",
            " [ 0.08257809]\n",
            " [-0.02298774]\n",
            " [ 0.19228337]\n",
            " [-0.05378803]\n",
            " [ 0.11479326]\n",
            " [-0.08572732]\n",
            " [ 0.07512266]\n",
            " [-0.07241275]\n",
            " [-0.11239339]\n",
            " [-0.01764989]\n",
            " [-0.00264561]\n",
            " [ 0.14428186]\n",
            " [ 0.01140611]\n",
            " [ 0.03925062]\n",
            " [ 0.00455833]\n",
            " [-0.14535521]\n",
            " [ 0.02122581]\n",
            " [-0.0336757 ]\n",
            " [-0.09698093]\n",
            " [-0.0465395 ]\n",
            " [-0.05564685]\n",
            " [ 0.15774633]\n",
            " [ 0.1100466 ]\n",
            " [-0.1940692 ]\n",
            " [-0.16462241]\n",
            " [-0.04296012]\n",
            " [ 0.05859482]\n",
            " [ 0.1025913 ]\n",
            " [-0.12813982]\n",
            " [ 0.03343733]\n",
            " [ 0.0937581 ]\n",
            " [-0.07843308]\n",
            " [ 0.17370357]\n",
            " [ 0.12467393]\n",
            " [-0.02832507]\n",
            " [ 0.05147001]\n",
            " [ 0.079885  ]\n",
            " [ 0.00098249]\n",
            " [ 0.04305971]\n",
            " [ 0.06827873]\n",
            " [ 0.00800575]\n",
            " [ 0.06249518]\n",
            " [ 0.08851393]\n",
            " [-0.01156387]\n",
            " [ 0.017748  ]\n",
            " [-0.02462326]\n",
            " [ 0.04817148]\n",
            " [ 0.02478664]\n",
            " [-0.05018728]\n",
            " [-0.11651197]\n",
            " [-0.06241781]\n",
            " [ 0.13388806]\n",
            " [ 0.02810952]\n",
            " [ 0.01460383]\n",
            " [-0.03925389]\n",
            " [-0.06740711]\n",
            " [-0.08962184]\n",
            " [ 0.07272945]\n",
            " [-0.00799549]\n",
            " [ 0.2041016 ]\n",
            " [ 0.05669838]]\n",
            "K-Means converged in 14 iterations.\n",
            "{'cluster_centers': array([[ 0.04713466],\n",
            "       [-0.25546382],\n",
            "       [ 0.37391421],\n",
            "       [-0.50990909],\n",
            "       [ 0.63932878],\n",
            "       [-0.0849238 ],\n",
            "       [ 0.1804491 ],\n",
            "       [ 0.94725427],\n",
            "       [-0.01494565],\n",
            "       [-0.36325306],\n",
            "       [-0.14493202],\n",
            "       [ 0.29315443],\n",
            "       [ 0.49369523],\n",
            "       [-0.71522105],\n",
            "       [ 0.10455789],\n",
            "       [-0.306375  ],\n",
            "       [-0.45303114],\n",
            "       [ 0.23188076],\n",
            "       [-0.05355645],\n",
            "       [-0.20056913],\n",
            "       [ 0.56774879],\n",
            "       [ 0.43580205],\n",
            "       [ 0.08179394],\n",
            "       [ 0.33549158],\n",
            "       [ 0.86734051],\n",
            "       [ 0.01669037],\n",
            "       [-0.12914792],\n",
            "       [ 0.1546631 ],\n",
            "       [-0.5551765 ],\n",
            "       [-0.66280663],\n",
            "       [ 0.26355027],\n",
            "       [ 0.77653587],\n",
            "       [ 0.13203153],\n",
            "       [-0.17351416],\n",
            "       [-0.23007005],\n",
            "       [-0.33823772],\n",
            "       [-0.41887091],\n",
            "       [ 0.69699526],\n",
            "       [-0.10225766],\n",
            "       [ 0.00377755],\n",
            "       [ 0.31279552],\n",
            "       [-0.47487553],\n",
            "       [-0.0310166 ],\n",
            "       [ 0.39598739],\n",
            "       [-0.28766356],\n",
            "       [-0.63254189],\n",
            "       [ 0.20605909],\n",
            "       [ 0.61523314],\n",
            "       [-0.38502733],\n",
            "       [ 0.4573063 ],\n",
            "       [ 0.03624211],\n",
            "       [-0.06161435],\n",
            "       [ 0.52169812],\n",
            "       [-0.69155294],\n",
            "       [-0.52481288],\n",
            "       [-0.27245424],\n",
            "       [-0.3244637 ],\n",
            "       [ 0.34417845],\n",
            "       [ 0.07279489],\n",
            "       [-0.12167574],\n",
            "       [ 0.58865798],\n",
            "       [ 0.05390969],\n",
            "       [-0.2190312 ],\n",
            "       [ 0.66150829],\n",
            "       [-0.07115085],\n",
            "       [ 0.12001461],\n",
            "       [-0.04679389],\n",
            "       [ 0.21785521],\n",
            "       [ 0.16277635],\n",
            "       [ 0.25328952],\n",
            "       [-0.16287582],\n",
            "       [-0.57303423],\n",
            "       [-0.19701135],\n",
            "       [-0.43957142],\n",
            "       [ 0.28094661],\n",
            "       [ 0.08867895],\n",
            "       [ 0.14459667],\n",
            "       [ 0.36662205],\n",
            "       [-0.24783473],\n",
            "       [ 0.30076712],\n",
            "       [ 0.32077371],\n",
            "       [-0.02170119],\n",
            "       [-0.09210872],\n",
            "       [ 0.47619385],\n",
            "       [ 0.44643646],\n",
            "       [ 0.40979126],\n",
            "       [-0.00186028],\n",
            "       [-0.48498303],\n",
            "       [ 0.02262975],\n",
            "       [ 0.55341169],\n",
            "       [-0.4646349 ],\n",
            "       [-0.19038613],\n",
            "       [-0.20881974],\n",
            "       [ 0.19375558],\n",
            "       [ 0.51038802],\n",
            "       [ 0.06151555],\n",
            "       [-0.03920106],\n",
            "       [-0.2960718 ],\n",
            "       [ 0.00907471],\n",
            "       [-0.53671426],\n",
            "       [-0.26311269],\n",
            "       [-0.13352641],\n",
            "       [-0.11381183],\n",
            "       [ 0.04115995],\n",
            "       [ 0.42555094],\n",
            "       [ 0.57956034],\n",
            "       [ 0.62576389],\n",
            "       [ 0.09891885],\n",
            "       [ 0.11099954],\n",
            "       [ 0.03042203],\n",
            "       [ 0.5997957 ],\n",
            "       [ 0.1677683 ],\n",
            "       [-0.01002736],\n",
            "       [ 0.40309522],\n",
            "       [ 0.22505336],\n",
            "       [ 0.2700649 ],\n",
            "       [-0.3577139 ],\n",
            "       [-0.06665641],\n",
            "       [-0.45747846],\n",
            "       [ 0.46624613],\n",
            "       [ 0.2583667 ],\n",
            "       [-0.09753115],\n",
            "       [-0.08002934],\n",
            "       [-0.15149063],\n",
            "       [-0.31684387],\n",
            "       [ 0.35238725],\n",
            "       [-0.18033566],\n",
            "       [-0.33072978]]), 'assignments': array([ 48,  15,  65, 117,  93,   8,  22, 116,  82,  59,  34,  38,  40,\n",
            "        70,  94,  70, 102, 103,  59, 107,  56,  10,  51,  14,  32,  43,\n",
            "        65,  32,  68,  50,  46,  86,  76,  43,  39,   5,   5,  98,   0,\n",
            "        15,  15,  18,  16,  20,  33, 101,  62,  20,  73,  71, 124, 123,\n",
            "       100,  81,  78, 106,  86,  25, 117, 111, 114,  91,  39,  95, 114,\n",
            "        38,  18,  27,  38,   0,  42,  88,  92,  54,   8, 113,  52,  66,\n",
            "        58,   5,  44,  38, 121,  72,   2,  90,  59,  38,  64,  32,  98,\n",
            "        85,  22, 100, 120,  57,  79,  82,  61,  98,  44,  33,  36, 104,\n",
            "        62,  75,  70,  12,  66,  96,   4,  67, 100,  86, 116,  57,  75,\n",
            "       103,  26,  64,  86, 122,  61,  33,  89,  59, 118,   5,  81,  41,\n",
            "       111,  51,  51,  98,  40,  27, 109,  76, 122,  65,  51, 108,  61,\n",
            "        47,  86,  38,  75,  63,  92,  89, 102,  26,   5,  82,  66,  73,\n",
            "        50,  69,  41,  56, 121,  23,  58,  50,  62,  60,  88,  39, 115,\n",
            "         5, 120, 112,  79,  11,  25,  86,   3,  61, 108,  55,  78, 112,\n",
            "        91,  81,  96,  21,  75,  57,  51,  68,   8,  92,  18, 123,  38,\n",
            "        70,  68, 114,  74,  98,  81,  12,  75,  51,  64, 117,  97,  72,\n",
            "        81,  25,  14,  10, 109,  16,  34,  82,  27,  61,  40,  58,   6,\n",
            "        25,  18,  70,  53,  44, 120, 108,  38,  21,  44,  77,  33,  62,\n",
            "        66,  39,  19,  76,  82,  49,  31,  91,   5,  13,  14,  23,  42,\n",
            "        56,  94,  18,  37,  78,  35,  65,   2, 114,  81, 115,   0,  84,\n",
            "        81, 117,  75,  46,  35,  72,  42,  86,  10,  25,  66,  51,  98,\n",
            "        80,  22,  55,  58,  18,  91,  27,  11, 125,  61,  25,  22,  58,\n",
            "        46,  32,  81,  63,  58,  86, 117,  95,  17,  48,  76,  23,  19,\n",
            "        50,  35,  54,  74,   1,  58, 117,  66,  87,  82,  32,  55,   6,\n",
            "        30,   7,  59, 121,  10,  81,  96, 109,  30, 112, 122,  14, 123,\n",
            "        59,  99,  47,  64,  17,  88,  92, 107,  18,  64,  10,  80, 112,\n",
            "        66,  81,  34,  46,  36,  45,  92,  82,  77,   9,  68, 105,  35,\n",
            "        43,  27,  50,  32,  54, 121,  18,  77, 116,  14,  34,  42, 107,\n",
            "        74, 127,  42, 103,  51,  86,  33,  22, 100, 111,  75, 122,  79,\n",
            "         6,  61, 104,  74, 126,  79,  59,  33,  41,  19,  26,  86,   9,\n",
            "        25, 103,  50,  78,  48, 119,  61,  51, 109,  22,   7, 115,  58,\n",
            "         0,  40,  51,  49,  83,  17,  18,  15,  42,   1, 118,  93,  26,\n",
            "       102,  25,  32,  96, 109,  67,  66,  86,  76, 112, 110,  29,   5,\n",
            "       121,  34,  56,  98, 102,  25,  58,  96,  42,  95,  55,  36,   6,\n",
            "        24,  17,  65,  59,  15,  87,  96,  73,  58,  39,  21,  86,  18,\n",
            "       107,  18,  56,  75,   8,  42,  50,   8,  10,  38,  26,  22,  86,\n",
            "        68, 122,  61,  58,  82,  55,  82, 117,  44, 121, 103,  33, 109,\n",
            "        39,  39,  80,  26, 113,  58,  97, 103,  28,  95,  15,  86,  47,\n",
            "        76, 101,  48,  55, 101, 112], dtype=int32), 'data_shape': (500,)} [[ 0.04713466]\n",
            " [-0.25546382]\n",
            " [ 0.37391421]\n",
            " [-0.50990909]\n",
            " [ 0.63932878]\n",
            " [-0.0849238 ]\n",
            " [ 0.1804491 ]\n",
            " [ 0.94725427]\n",
            " [-0.01494565]\n",
            " [-0.36325306]\n",
            " [-0.14493202]\n",
            " [ 0.29315443]\n",
            " [ 0.49369523]\n",
            " [-0.71522105]\n",
            " [ 0.10455789]\n",
            " [-0.306375  ]\n",
            " [-0.45303114]\n",
            " [ 0.23188076]\n",
            " [-0.05355645]\n",
            " [-0.20056913]\n",
            " [ 0.56774879]\n",
            " [ 0.43580205]\n",
            " [ 0.08179394]\n",
            " [ 0.33549158]\n",
            " [ 0.86734051]\n",
            " [ 0.01669037]\n",
            " [-0.12914792]\n",
            " [ 0.1546631 ]\n",
            " [-0.5551765 ]\n",
            " [-0.66280663]\n",
            " [ 0.26355027]\n",
            " [ 0.77653587]\n",
            " [ 0.13203153]\n",
            " [-0.17351416]\n",
            " [-0.23007005]\n",
            " [-0.33823772]\n",
            " [-0.41887091]\n",
            " [ 0.69699526]\n",
            " [-0.10225766]\n",
            " [ 0.00377755]\n",
            " [ 0.31279552]\n",
            " [-0.47487553]\n",
            " [-0.0310166 ]\n",
            " [ 0.39598739]\n",
            " [-0.28766356]\n",
            " [-0.63254189]\n",
            " [ 0.20605909]\n",
            " [ 0.61523314]\n",
            " [-0.38502733]\n",
            " [ 0.4573063 ]\n",
            " [ 0.03624211]\n",
            " [-0.06161435]\n",
            " [ 0.52169812]\n",
            " [-0.69155294]\n",
            " [-0.52481288]\n",
            " [-0.27245424]\n",
            " [-0.3244637 ]\n",
            " [ 0.34417845]\n",
            " [ 0.07279489]\n",
            " [-0.12167574]\n",
            " [ 0.58865798]\n",
            " [ 0.05390969]\n",
            " [-0.2190312 ]\n",
            " [ 0.66150829]\n",
            " [-0.07115085]\n",
            " [ 0.12001461]\n",
            " [-0.04679389]\n",
            " [ 0.21785521]\n",
            " [ 0.16277635]\n",
            " [ 0.25328952]\n",
            " [-0.16287582]\n",
            " [-0.57303423]\n",
            " [-0.19701135]\n",
            " [-0.43957142]\n",
            " [ 0.28094661]\n",
            " [ 0.08867895]\n",
            " [ 0.14459667]\n",
            " [ 0.36662205]\n",
            " [-0.24783473]\n",
            " [ 0.30076712]\n",
            " [ 0.32077371]\n",
            " [-0.02170119]\n",
            " [-0.09210872]\n",
            " [ 0.47619385]\n",
            " [ 0.44643646]\n",
            " [ 0.40979126]\n",
            " [-0.00186028]\n",
            " [-0.48498303]\n",
            " [ 0.02262975]\n",
            " [ 0.55341169]\n",
            " [-0.4646349 ]\n",
            " [-0.19038613]\n",
            " [-0.20881974]\n",
            " [ 0.19375558]\n",
            " [ 0.51038802]\n",
            " [ 0.06151555]\n",
            " [-0.03920106]\n",
            " [-0.2960718 ]\n",
            " [ 0.00907471]\n",
            " [-0.53671426]\n",
            " [-0.26311269]\n",
            " [-0.13352641]\n",
            " [-0.11381183]\n",
            " [ 0.04115995]\n",
            " [ 0.42555094]\n",
            " [ 0.57956034]\n",
            " [ 0.62576389]\n",
            " [ 0.09891885]\n",
            " [ 0.11099954]\n",
            " [ 0.03042203]\n",
            " [ 0.5997957 ]\n",
            " [ 0.1677683 ]\n",
            " [-0.01002736]\n",
            " [ 0.40309522]\n",
            " [ 0.22505336]\n",
            " [ 0.2700649 ]\n",
            " [-0.3577139 ]\n",
            " [-0.06665641]\n",
            " [-0.45747846]\n",
            " [ 0.46624613]\n",
            " [ 0.2583667 ]\n",
            " [-0.09753115]\n",
            " [-0.08002934]\n",
            " [-0.15149063]\n",
            " [-0.31684387]\n",
            " [ 0.35238725]\n",
            " [-0.18033566]\n",
            " [-0.33072978]]\n",
            "K-Means converged in 5 iterations.\n",
            "C-step #0 has finished.\n",
            "Lagrange multipliers have been updated.\n",
            "Train err: 0.00%, train loss: 0.00018963509466221137\n",
            "TEST ERR: 1.31%, test loss: 0.05724179278183081\n",
            "Compressed_params: 266084\n",
            "Compression_ratio: 4.525265898198066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Ybu28hlpuA"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd9Y5YEKJCYh"
      },
      "source": [
        "### Mixing pruning, low rank, and quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReZIPzn3I8L8"
      },
      "source": [
        "net = load_reference_lenet300()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "\n",
        "compression_tasks = {\n",
        "    Param(layers[0], device): (AsVector, ConstraintL0Pruning(kappa=5000), 'pruning'),\n",
        "    Param(layers[1], device): (AsIs, LowRank(target_rank=9, conv_scheme=None), 'low-rank'),\n",
        "    Param(layers[2], device): (AsVector, AdaptiveQuantization(k=2), 'quant')\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()\n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08XSsmyEJG08"
      },
      "source": [
        "### Additive combination of Quantization and Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nO4XSlpI-48"
      },
      "source": [
        "net = load_reference_lenet300()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "\n",
        "compression_tasks = {\n",
        "    Param(layers, device): [\n",
        "        (AsVector, ConstraintL0Pruning(kappa=2662), 'pruning'),\n",
        "        (AsVector, AdaptiveQuantization(k=2), 'quant')\n",
        "    ]\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()\n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj7OJM3jKqW6"
      },
      "source": [
        "### Low-rank compression with automatic rank selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyDPsLAVKq9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18cf016-74c2-4a77-c4f9-42b0fcc8e090"
      },
      "source": [
        "parameters={\"batch_size\": 2**8, \"lr\": 0.2, \"gamma\":0.9, \"step_size\": 2, \"momentum\": 0.9, \"weight_decay\": 0, \"nesterov\": True, \"epochs_per_early_stop_check\": 3}\n",
        "net = Net().cuda()\n",
        "net.load_state_dict(torch.load(\"/content/state_dicts/\" + str(parameters) + \".pt\"))\n",
        "\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "# \n",
        "alpha=1e-9\n",
        "compression_tasks = {\n",
        "    Param(layers[0], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[0], normalize=True), \"layer1_lr\"),\n",
        "    Param(layers[1], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[1], normalize=True), \"layer2_lr\"),\n",
        "    Param(layers[2], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[2], normalize=True), \"layer3_lr\")\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()\n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from retrieve 174.84155325678307\n",
            "from retrieve 69.29116793712554\n",
            "from retrieve 32.8433945048008\n",
            "for this layer, selected rank is 0, normalized w=1.000, true w=174.842, ()=0.000, w-()=1.000e+00,  should be at least 3.56e-05\n",
            "for this layer, selected rank is 0, normalized w=1.000, true w=69.291, ()=0.000, w-()=1.000e+00,  should be at least 5.33e-06\n",
            "for this layer, selected rank is 0, normalized w=1.000, true w=32.843, ()=0.000, w-()=1.000e+00,  should be at least 6.86e-07\n",
            "Direct compression has been performed.\n",
            "Train err: 81.84%, train loss: 1.6619303559148035\n",
            "TEST ERR: 81.76%, test loss: 1.662122472322304\n",
            "9e-05\n",
            "L-step #0 with lr: 0.70000\n",
            "\tepoch #0 is finished.\n",
            "\t  avg. train loss: 0.012653\n",
            "\tepoch #1 is finished.\n",
            "\t  avg. train loss: 0.012555\n",
            "from retrieve 171.86938748091544\n",
            "from retrieve 68.25885266920355\n",
            "from retrieve 32.485757136050466\n",
            "L-step #0 has finished.\n",
            "for this layer, selected rank is 6, normalized w=1.000, true w=171.869, ()=0.236, w-()=7.640e-01,  should be at least 3.56e-05\n",
            "for this layer, selected rank is 16, normalized w=1.000, true w=68.259, ()=0.654, w-()=3.464e-01,  should be at least 5.32e-06\n",
            "for this layer, selected rank is 5, normalized w=1.000, true w=32.486, ()=1.000, w-()=0.000e+00,  should be at least 6.87e-07\n",
            "C-step #0 has finished.\n",
            "Lagrange multipliers have been updated.\n",
            "Train err: 3.77%, train loss: 0.18145802300567776\n",
            "TEST ERR: 4.46%, test loss: 0.23875931323427851\n",
            "Compressed_params: 13429\n",
            "Compression_ratio: 19.265120312161283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3FIQ2WWAfGw"
      },
      "source": [
        "### ScaledBinaryQuantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeESVmskORN8"
      },
      "source": [
        "from lc.compression_types import ScaledBinaryQuantization\n",
        "net = load_reference_lenet300()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "\n",
        "compression_tasks = {\n",
        "    Param(layers[0], device): (AsVector, ScaledBinaryQuantization(), 'layer0_quant'),\n",
        "    Param(layers[1], device): (AsVector, ScaledBinaryQuantization(), 'layer1_quant'),\n",
        "    Param(layers[2], device): (AsVector, ScaledBinaryQuantization(), 'layer2_quant')\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()\n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJlVuYlAAeqh"
      },
      "source": [
        "### ScaledTernaryQuantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWbz-VekApSw"
      },
      "source": [
        "from lc.compression_types import ScaledTernaryQuantization\n",
        "net = load_reference_lenet300()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "\n",
        "compression_tasks = {\n",
        "    Param(layers[0], device): (AsVector, ScaledTernaryQuantization(), 'layer0_quant'),\n",
        "    Param(layers[1], device): (AsVector, ScaledTernaryQuantization(), 'layer1_quant'),\n",
        "    Param(layers[2], device): (AsVector, ScaledTernaryQuantization(), 'layer2_quant')\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run() \n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}